import json
import os
import re
import warnings
import torch
import numpy as np


class AIIA_Podcast_Stitcher:
    """
    å°†åˆ†è½¨ç”Ÿæˆçš„å¤šè§’è‰²éŸ³é¢‘æŒ‰åŸå§‹å¯¹è¯é¡ºåºç²¾ç¡®æ‹¼æ¥ã€‚
    
    åˆ©ç”¨ ASR è¯çº§æ—¶é—´æˆ³æ‰¾åˆ°æ¯å¥è¯åœ¨éŸ³é¢‘ä¸­çš„è¾¹ç•Œï¼Œåˆ‡åˆ†åäº¤é”™æ‹¼æ¥ã€‚
    """

    NODE_NAME = "AIIA Podcast Stitcher"

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "split_map": ("STRING", {"forceInput": True}),
                "audio_A": ("AUDIO",),
                "audio_B": ("AUDIO",),
                "asr_A": ("ASR_RESULT",),
                "asr_B": ("ASR_RESULT",),
            },
            "optional": {
                "gap_duration": ("FLOAT", {
                    "default": 0.25, "min": 0.0, "max": 2.0, "step": 0.05,
                    "tooltip": "è¯´è¯äººäº¤æ›¿æ—¶æ’å…¥çš„è¿‡æ¸¡æ—¶é•¿ï¼ˆç§’ï¼‰"
                }),
                "padding": ("FLOAT", {
                    "default": 0.10, "min": 0.0, "max": 0.5, "step": 0.01,
                    "tooltip": "æ¯ä¸ªåˆ‡ç‰‡å‰åä¿ç•™çš„å‘¼å¸/å°¾éŸ³ä½™é‡ï¼ˆç§’ï¼‰"
                }),
                "fade_ms": ("INT", {
                    "default": 30, "min": 5, "max": 100, "step": 5,
                    "tooltip": "åˆ‡ç‰‡é¦–å°¾çš„ä½™å¼¦æ·¡å…¥æ·¡å‡ºæ—¶é•¿ï¼ˆæ¯«ç§’ï¼‰ï¼Œè¶Šé•¿è¶Šå¹³æ»‘"
                }),
                "use_vad": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "å¯ç”¨ Silero VAD æ¨¡å‹ç²¾ç¡®æ£€æµ‹è¯­éŸ³è¾¹ç•Œï¼ˆé¦–æ¬¡ä½¿ç”¨è‡ªåŠ¨ä¸‹è½½ ~2MB æ¨¡å‹ï¼‰"
                }),
                "use_forced_align": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "å¯ç”¨ MMS Forced Alignment å­—çº§å¼ºåˆ¶å¯¹é½ï¼ˆéœ€è¦ ~1.2GB æ¨¡å‹ï¼Œç²¾åº¦æœ€é«˜ï¼‰"
                }),
            }
        }

    RETURN_TYPES = ("AUDIO", "STRING",)
    RETURN_NAMES = ("audio", "segments_info",)
    FUNCTION = "stitch"
    CATEGORY = "AIIA/Podcast"

    def _audio_to_numpy(self, audio: dict) -> tuple:
        """å°† ComfyUI AUDIO è½¬ä¸º numpy æ•°ç»„å’Œé‡‡æ ·ç‡ã€‚"""
        waveform = audio["waveform"]
        sr = audio["sample_rate"]

        if waveform.ndim == 3:
            wav = waveform[0]
        else:
            wav = waveform

        if wav.ndim == 2 and wav.shape[0] > 1:
            wav = wav.mean(dim=0)
        elif wav.ndim == 2:
            wav = wav.squeeze(0)

        return wav.cpu().numpy().astype(np.float32), sr

    def _find_sentence_boundaries(self, asr_words: list, sentences: list, total_duration: float) -> list:
        """
        å°† ASR è¯çº§æ—¶é—´æˆ³ä¸åŸå§‹å¥å­åˆ—è¡¨å¯¹é½ï¼Œæ‰¾åˆ°æ¯å¥è¯åœ¨éŸ³é¢‘ä¸­çš„æ—¶é—´èŒƒå›´ã€‚
        
        ä¸‰å±‚åŒ¹é…ç­–ç•¥ï¼š
        1. ç²¾ç¡®å­ä¸²åŒ¹é…ï¼ˆå»æ ‡ç‚¹åï¼‰
        2. ç¼–è¾‘è·ç¦»æ¨¡ç³ŠåŒ¹é…ï¼ˆæ»‘åŠ¨çª—å£ï¼Œå®¹å¿ ASR é”™å­—/æ¼å­—ï¼‰
        3. é—´éš™å¡«è¡¥ / ç­‰åˆ†å›é€€
        """
        log = f"[{self.NODE_NAME}]"

        if not asr_words:
            print(f"{log} ASR ç»“æœä¸ºç©ºï¼Œä½¿ç”¨ç­‰åˆ†ç­–ç•¥")
            return self._fallback_equal_split(sentences, total_duration)

        if not sentences:
            return []

        # æ„å»º ASR æ–‡æœ¬å’Œå­—ç¬¦åˆ°è¯ç´¢å¼•çš„æ˜ å°„
        asr_full_text = ""
        char_to_word_idx = []  # char_to_word_idx[i] = è¯¥å­—ç¬¦å±äºå“ªä¸ª word
        for word_idx, w in enumerate(asr_words):
            word_text = w["word"]
            for ch in word_text:
                char_to_word_idx.append(word_idx)
            asr_full_text += word_text

        print(f"{log} ASR å…¨æ–‡ ({len(asr_full_text)} å­—): {asr_full_text[:100]}...")

        # ä¸ºæ¯å¥è¯æ‰¾åˆ°åœ¨ ASR æ–‡æœ¬ä¸­çš„åŒ¹é…ä½ç½®
        boundaries = []
        search_start = 0  # ä¿è¯é¡ºåºåŒ¹é…

        for sent_idx, sentence in enumerate(sentences):
            # æ¸…ç†å¥å­æ–‡æœ¬ï¼ˆå»é™¤æ ‡ç‚¹ç¬¦å·å’Œç©ºæ ¼ï¼Œä¸ ASR è¾“å‡ºå¯¹é½ï¼‰
            clean_sent = self._clean_text_for_matching(sentence)

            if not clean_sent:
                print(f"{log} å¥å­ {sent_idx} æ¸…ç†åä¸ºç©º: '{sentence}'")
                boundaries.append(None)
                continue

            # === ç¬¬ 1 å±‚ï¼šç²¾ç¡®å­ä¸²åŒ¹é… ===
            match_pos = asr_full_text.find(clean_sent, search_start)

            if match_pos != -1:
                match_end = match_pos + len(clean_sent) - 1
                match_quality = "ç²¾ç¡®"
            else:
                # === ç¬¬ 2 å±‚ï¼šç¼–è¾‘è·ç¦»æ¨¡ç³ŠåŒ¹é… ===
                match_pos, match_end, edit_dist = self._fuzzy_find(
                    asr_full_text, clean_sent, search_start
                )

                if match_pos != -1:
                    match_quality = f"æ¨¡ç³Š(ed={edit_dist})"
                else:
                    print(f"{log} å¥å­ {sent_idx} æ— æ³•åŒ¹é…: '{clean_sent[:30]}...'")
                    boundaries.append(None)
                    continue

            # æ˜ å°„å­—ç¬¦ä½ç½®åˆ°è¯ç´¢å¼•
            start_word_idx = char_to_word_idx[match_pos] if match_pos < len(char_to_word_idx) else len(asr_words) - 1
            end_word_idx = char_to_word_idx[min(match_end, len(char_to_word_idx) - 1)]

            start_time = asr_words[start_word_idx]["start"]
            end_time = asr_words[end_word_idx]["end"]

            print(f"{log} å¥å­ {sent_idx} [{match_quality}]: "
                  f"'{clean_sent[:15]}' â†’ pos={match_pos}-{match_end}, "
                  f"time={start_time:.2f}-{end_time:.2f}s")

            boundaries.append({
                "start": start_time,
                "end": end_time,
                "start_word_idx": start_word_idx,
                "end_word_idx": end_word_idx,
            })

            # æ›´æ–°æœç´¢èµ·ç‚¹
            search_start = match_end + 1

        # å¡«è¡¥æœªåŒ¹é…çš„å¥å­ï¼ˆä½¿ç”¨å‰åå¥å­çš„æ—¶é—´æ’å€¼ï¼‰
        boundaries = self._fill_missing_boundaries(boundaries, asr_words, total_duration)

        # æ‰©å±•è¾¹ç•Œåˆ°å¥é—´é—´éš™çš„ä¸­ç‚¹ï¼ˆé¿å…æˆªæ–­å°¾éŸ³ï¼‰
        boundaries = self._expand_to_midpoints(boundaries, total_duration)

        return boundaries

    @staticmethod
    def _edit_distance(s1: str, s2: str) -> int:
        """è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²çš„ç¼–è¾‘è·ç¦»ï¼ˆLevenshtein distanceï¼‰ï¼Œä½¿ç”¨ç©ºé—´ä¼˜åŒ–çš„ DPã€‚"""
        m, n = len(s1), len(s2)
        if m == 0:
            return n
        if n == 0:
            return m

        # åªéœ€ä¸¤è¡Œ
        prev = list(range(n + 1))
        curr = [0] * (n + 1)

        for i in range(1, m + 1):
            curr[0] = i
            for j in range(1, n + 1):
                if s1[i - 1] == s2[j - 1]:
                    curr[j] = prev[j - 1]
                else:
                    curr[j] = 1 + min(prev[j], curr[j - 1], prev[j - 1])
            prev, curr = curr, prev

        return prev[n]

    def _fuzzy_find(self, haystack: str, needle: str, search_start: int = 0,
                    max_error_ratio: float = 0.4) -> tuple:
        """
        åœ¨ haystack ä¸­ä» search_start å¼€å§‹ï¼Œç”¨æ»‘åŠ¨çª—å£+ç¼–è¾‘è·ç¦»æ‰¾åˆ°ä¸ needle æœ€ç›¸ä¼¼çš„å­ä¸²ã€‚
        
        å‚æ•°:
            haystack: ASR å…¨æ–‡
            needle: å¾…åŒ¹é…çš„åŸå§‹å¥å­ï¼ˆå·²å»æ ‡ç‚¹ï¼‰
            search_start: æœç´¢èµ·å§‹ä½ç½®
            max_error_ratio: å…è®¸çš„æœ€å¤§é”™è¯¯ç‡ï¼ˆç¼–è¾‘è·ç¦» / needle é•¿åº¦ï¼‰
        
        è¿”å›:
            (match_pos, match_end, edit_distance)  æˆ–  (-1, -1, -1) è¡¨ç¤ºå¤±è´¥
        """
        needle_len = len(needle)
        if needle_len == 0:
            return (-1, -1, -1)

        max_errors = int(needle_len * max_error_ratio)
        remaining = haystack[search_start:]
        remaining_len = len(remaining)

        if remaining_len == 0:
            return (-1, -1, -1)

        best_pos = -1
        best_end = -1
        best_dist = needle_len + 1  # åˆå§‹åŒ–ä¸ºä¸€ä¸ªå¤§å€¼

        # å°è¯•å¤šç§çª—å£å¤§å°ï¼ˆneedle é•¿åº¦çš„ Â±30%ï¼‰ï¼Œå¤„ç† ASR æ¼å­—/å¤šå­—çš„æƒ…å†µ
        window_sizes = set()
        for ratio in [1.0, 0.85, 0.9, 0.95, 1.05, 1.1, 1.15, 1.2]:
            ws = max(1, int(needle_len * ratio))
            if ws <= remaining_len:
                window_sizes.add(ws)

        # é™åˆ¶æœç´¢èŒƒå›´ä»¥é¿å… O(nÂ²) çˆ†ç‚¸
        # åœ¨åˆç†çš„æœç´¢èŒƒå›´å†…ï¼šä» search_start å¼€å§‹ï¼Œæœ€å¤šæœåˆ° needle é•¿åº¦çš„ 3 å€
        max_search_len = min(remaining_len, needle_len * 3 + 20)

        for window_size in sorted(window_sizes):
            for i in range(0, max_search_len - window_size + 1):
                candidate = remaining[i:i + window_size]
                dist = self._edit_distance(needle, candidate)

                if dist < best_dist:
                    best_dist = dist
                    best_pos = search_start + i
                    best_end = search_start + i + window_size - 1

                    # å¦‚æœç¼–è¾‘è·ç¦»ä¸º 0 æˆ– 1ï¼Œå¯ä»¥æå‰é€€å‡º
                    if dist <= 1:
                        break

            if best_dist <= 1:
                break

        # åªæ¥å—é”™è¯¯ç‡åœ¨é˜ˆå€¼å†…çš„åŒ¹é…
        if best_dist <= max_errors:
            return (best_pos, best_end, best_dist)
        else:
            return (-1, -1, -1)

    def _clean_text_for_matching(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬ç”¨äºä¸ ASR è¾“å‡ºåŒ¹é…ï¼šå»é™¤æ ‡ç‚¹ã€ç©ºæ ¼ã€è‹±æ–‡è½¬å°å†™ã€‚"""
        import re
        # å»é™¤å¸¸è§ä¸­è‹±æ–‡æ ‡ç‚¹å’Œç©ºæ ¼
        cleaned = re.sub(r'[ï¼Œã€‚ï¼ï¼Ÿã€ï¼›ï¼š""''ã€Œã€ã€ã€‘ï¼ˆï¼‰ã€Šã€‹\s,\.!?\-\;\:\"\'\(\)\[\]\{\}â€¦â€”~ï½Â·]', '', text)
        # è‹±æ–‡è½¬å°å†™ï¼ˆASR å¯èƒ½è¾“å‡ºä¸åŒå¤§å°å†™ï¼‰
        cleaned = cleaned.lower()
        return cleaned

    def _fallback_equal_split(self, sentences: list, total_duration: float) -> list:
        """å›é€€ç­–ç•¥ï¼šæŒ‰å¥å­å­—ç¬¦æ•°ç­‰æ¯”ä¾‹åˆ†é…æ—¶é—´ã€‚"""
        if not sentences:
            return []

        total_chars = sum(len(s) for s in sentences)
        if total_chars == 0:
            segment_duration = total_duration / len(sentences)
            return [{"start": i * segment_duration, "end": (i + 1) * segment_duration}
                    for i in range(len(sentences))]

        boundaries = []
        current_time = 0.0
        for sent in sentences:
            ratio = len(sent) / total_chars
            duration = ratio * total_duration
            boundaries.append({
                "start": round(current_time, 3),
                "end": round(current_time + duration, 3),
            })
            current_time += duration

        return boundaries

    def _fill_missing_boundaries(self, boundaries: list, asr_words: list, total_duration: float) -> list:
        """å¡«è¡¥æœªèƒ½åŒ¹é…çš„å¥å­è¾¹ç•Œã€‚"""
        filled = list(boundaries)

        for i in range(len(filled)):
            if filled[i] is not None:
                continue

            # æ‰¾å‰ä¸€ä¸ªå·²çŸ¥è¾¹ç•Œ
            prev_end = 0.0
            for j in range(i - 1, -1, -1):
                if filled[j] is not None:
                    prev_end = filled[j]["end"]
                    break

            # æ‰¾åä¸€ä¸ªå·²çŸ¥è¾¹ç•Œ
            next_start = total_duration
            for j in range(i + 1, len(filled)):
                if filled[j] is not None:
                    next_start = filled[j]["start"]
                    break

            # åœ¨ç©ºéš™ä¸­å‡åŒ€åˆ†é…
            gap_count = 0
            gap_start_idx = i
            for j in range(i, len(filled)):
                if filled[j] is None:
                    gap_count += 1
                else:
                    break

            gap_duration = (next_start - prev_end) / gap_count
            for k in range(gap_count):
                filled[gap_start_idx + k] = {
                    "start": round(prev_end + k * gap_duration, 3),
                    "end": round(prev_end + (k + 1) * gap_duration, 3),
                }

        return filled

    # ========== Forced Alignment (MMS_FA) ==========
    _fa_model = None
    _fa_tokenizer = None
    _fa_aligner = None

    @classmethod
    def _load_fa_model(cls):
        """æ‡’åŠ è½½ MMS_FA æ¨¡å‹ï¼ˆç±»çº§å•ä¾‹ï¼‰ã€‚ä¼˜å…ˆä» models/mms_fa/ è¯»å–æœ¬åœ°æƒé‡ã€‚"""
        if cls._fa_model is not None:
            return cls._fa_model, cls._fa_tokenizer, cls._fa_aligner
        try:
            from torchaudio.pipelines import MMS_FA as bundle

            # ç¡®ä¿æœ¬åœ°æ¨¡å‹æ–‡ä»¶åœ¨ hub cache ä¸­ï¼ˆsymlinkï¼‰
            import folder_paths
            local_model = os.path.join(folder_paths.models_dir, "mms_fa", "model.pt")
            hub_cache = os.path.join(os.path.expanduser("~"), ".cache", "torch", "hub", "checkpoints", "model.pt")
            if os.path.exists(local_model) and not os.path.exists(hub_cache):
                os.makedirs(os.path.dirname(hub_cache), exist_ok=True)
                os.symlink(local_model, hub_cache)
                print(f"[{cls.__name__}] å·²é“¾æ¥æœ¬åœ°æ¨¡å‹: {local_model} -> {hub_cache}")

            warnings.filterwarnings('ignore', message='.*forced_align has been deprecated.*')
            model = bundle.get_model().to('cuda' if torch.cuda.is_available() else 'cpu')
            tokenizer = bundle.get_tokenizer()
            aligner = bundle.get_aligner()
            cls._fa_model = model
            cls._fa_tokenizer = tokenizer
            cls._fa_aligner = aligner
            print(f"[{cls.__name__}] MMS Forced Alignment æ¨¡å‹åŠ è½½æˆåŠŸ")
            return model, tokenizer, aligner
        except Exception as e:
            print(f"[{cls.__name__}] MMS FA åŠ è½½å¤±è´¥: {e}")
            return None, None, None

    @staticmethod
    def _chinese_to_pinyin(text):
        """å°†ä¸­æ–‡æ–‡æœ¬è½¬ä¸ºæ‹¼éŸ³å­—ç¬¦ä¸²ï¼ˆMMS_FA åªæ¥å—å°å†™æ‹‰ä¸å­—ç¬¦ï¼‰ã€‚"""
        from pypinyin import lazy_pinyin, Style
        # å»é™¤æ ‡ç‚¹å’Œç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™ä¸­æ–‡ã€å­—æ¯ã€æ•°å­—ã€ç©ºæ ¼
        clean = re.sub(r'[^\u4e00-\u9fff\w\s]', '', text)
        if not clean.strip():
            return text.lower()
        # lazy_pinyin ä¼šæŠŠä¸­æ–‡è½¬æ‹¼éŸ³ï¼Œéä¸­æ–‡å­—ç¬¦åŸæ ·ä¿ç•™
        result = ' '.join(lazy_pinyin(clean, style=Style.NORMAL))
        # å…¨éƒ¨å°å†™ + åªä¿ç•™ MMS_FA tokenizer æ”¯æŒçš„å­—ç¬¦ [a-z, space, ', -]
        result = result.lower()
        result = re.sub(r"[^a-z\s'\-]", '', result)
        # åˆå¹¶å¤šä½™ç©ºæ ¼
        result = re.sub(r'\s+', ' ', result).strip()
        return result if result else 'a'

    def _forced_align_sentences(self, wav_np, sr, sentences):
        """
        å¯¹å®Œæ•´éŸ³é¢‘åš MMS Forced Alignmentï¼Œè¿”å›æ¯å¥çš„ç²¾ç¡® {start, end} æ—¶é—´ã€‚
        
        å·¥ä½œæµç¨‹ï¼š
        1. å°†æ‰€æœ‰å¥å­æ‹¼ä¸ºå®Œæ•´ pinyin è½¬å½•
        2. MMS_FA æ¨¡å‹ç”Ÿæˆ emission
        3. aligner åš CTC å¼ºåˆ¶å¯¹é½ï¼Œå¾—åˆ°æ¯ä¸ª word çš„ token_spans
        4. æ ¹æ®å¥å­â†’word æ˜ å°„è¿˜åŸæ¯å¥çš„ start/end
        """
        model, tokenizer, aligner = self._fa_model, self._fa_tokenizer, self._fa_aligner
        if model is None:
            return None
        
        log = f"[{self.NODE_NAME}]"
        
        # 1. å‡†å¤‡ pinyin è½¬å½•ï¼ˆä»¥ word ä¸ºå•ä½ï¼Œç”¨ç©ºæ ¼åˆ†éš”ï¼‰
        all_words = []       # pinyin words åˆ—è¡¨
        sentence_word_ranges = []  # æ¯å¥å¯¹åº”çš„ [start_word_idx, end_word_idx)
        
        for sent in sentences:
            pinyin_str = self._chinese_to_pinyin(sent)
            words = pinyin_str.split()
            if not words:
                words = ['a']  # å ä½ç¬¦ï¼Œé¿å…ç©ºå¥å­
            start_idx = len(all_words)
            all_words.extend(words)
            sentence_word_ranges.append((start_idx, len(all_words)))
        
        if not all_words:
            return None
            
        # 2. éŸ³é¢‘é‡é‡‡æ ·åˆ° 16kHzï¼ˆMMS_FA è¦æ±‚ï¼‰
        import torchaudio
        FA_SR = 16000
        wav_tensor = torch.from_numpy(wav_np).float().unsqueeze(0)  # [1, T]
        if sr != FA_SR:
            wav_tensor = torchaudio.functional.resample(wav_tensor, sr, FA_SR)
        
        device = next(model.parameters()).device
        wav_tensor = wav_tensor.to(device)
        
        # 3. æ¨¡å‹æ¨ç†
        try:
            with torch.inference_mode():
                emission, _ = model(wav_tensor)
            
            token_spans = aligner(emission[0], tokenizer(all_words))
        except Exception as e:
            print(f"{log} FA å¯¹é½å¤±è´¥: {e}")
            return None
        
        # 4. å°† token_spans æ˜ å°„å›æ¯å¥çš„æ—¶é—´èŒƒå›´
        num_frames = emission.size(1)
        ratio = wav_tensor.size(1) / num_frames / FA_SR  # frame â†’ ç§’
        
        results = []
        for sent_idx, (w_start, w_end) in enumerate(sentence_word_ranges):
            if w_start >= len(token_spans) or w_end > len(token_spans):
                # å›é€€ï¼šæ— å¯¹é½ç»“æœ
                results.append(None)
                continue
            
            # å¥å­çš„ç¬¬ä¸€ä¸ª word çš„ç¬¬ä¸€ä¸ª token â†’ start
            # å¥å­çš„æœ€åä¸€ä¸ª word çš„æœ€åä¸€ä¸ª token â†’ end
            first_spans = token_spans[w_start]
            last_spans = token_spans[w_end - 1]
            
            if not first_spans or not last_spans:
                results.append(None)
                continue
            
            t_start = first_spans[0].start * ratio
            t_end = last_spans[-1].end * ratio
            
            # è®¡ç®—å¯¹é½ç½®ä¿¡åº¦
            all_span_scores = []
            for wi in range(w_start, w_end):
                for s in token_spans[wi]:
                    all_span_scores.append(s.score)
            avg_score = sum(all_span_scores) / len(all_span_scores) if all_span_scores else 0
            
            results.append({
                'start': round(t_start, 4),
                'end': round(t_end, 4),
                'score': round(avg_score, 3)
            })
            print(f"{log} FA å¥å­ {sent_idx}: [{t_start:.3f}s - {t_end:.3f}s] score={avg_score:.3f} '{sentences[sent_idx][:20]}'")
        
        return results

    @staticmethod
    def _compute_iou(start1, end1, start2, end2):
        """è®¡ç®—ä¸¤ä¸ªæ—¶é—´åŒºé—´çš„ IoUï¼ˆIntersection over Unionï¼‰ã€‚"""
        inter_start = max(start1, start2)
        inter_end = min(end1, end2)
        intersection = max(0, inter_end - inter_start)
        union = max(end1, end2) - min(start1, start2)
        return round(intersection / union, 3) if union > 0 else 0.0

    # ========== Silero VAD ==========
    _vad_model = None
    _vad_utils = None

    @classmethod
    def _load_vad_model(cls):
        """æ‡’åŠ è½½ Silero VAD æ¨¡å‹ï¼ˆç±»çº§å•ä¾‹ï¼Œåªä¸‹è½½ä¸€æ¬¡ï¼‰ã€‚"""
        if cls._vad_model is not None:
            return cls._vad_model, cls._vad_utils
        try:
            model, utils = torch.hub.load(
                repo_or_dir='snakers4/silero-vad',
                model='silero_vad',
                force_reload=False,
                onnx=False
            )
            cls._vad_model = model
            cls._vad_utils = utils
            print(f"[{cls.__name__}] Silero VAD æ¨¡å‹åŠ è½½æˆåŠŸ")
            return model, utils
        except Exception as e:
            print(f"[{cls.__name__}] Silero VAD åŠ è½½å¤±è´¥: {e}")
            return None, None

    def _get_vad_timestamps(self, wav_np, sr, vad_model, vad_utils):
        """
        å¯¹å®Œæ•´éŸ³é¢‘è¿è¡Œ Silero VADï¼Œè¿”å›è¯­éŸ³åŒºé—´åˆ—è¡¨ [{start: float, end: float}, ...]ï¼ˆå•ä½ï¼šç§’ï¼‰ã€‚
        """
        get_speech_timestamps = vad_utils[0]
        
        # Silero VAD è¦æ±‚ 16kHz
        vad_sr = 16000
        wav_tensor = torch.from_numpy(wav_np).float()
        if sr != vad_sr:
            import torchaudio
            wav_tensor = torchaudio.functional.resample(wav_tensor, sr, vad_sr)
        
        # è¿è¡Œ VAD
        timestamps = get_speech_timestamps(
            wav_tensor, vad_model,
            sampling_rate=vad_sr,
            threshold=0.3,              # çµæ•åº¦ï¼ˆTTS å¹²å‡€éŸ³é¢‘å¯ä»¥ç•¥ä½ï¼‰
            min_speech_duration_ms=100,  # æœ€çŸ­è¯­éŸ³æ®µ 100ms
            min_silence_duration_ms=50,  # æœ€çŸ­é™éŸ³æ®µ 50ms
            speech_pad_ms=20,            # è¯­éŸ³ä¸¤ä¾§å¡«å…… 20ms
            return_seconds=True
        )
        
        return timestamps  # [{start: float, end: float}, ...]

    def _refine_with_vad(self, cut_start, cut_end, vad_timestamps, search_margin=0.2):
        """
        ç”¨ VAD åŒºé—´ç²¾ä¿® cut_start/cut_endã€‚
        
        ç­–ç•¥ï¼šæ‰¾åˆ°ä¸ [cut_start, cut_end] é‡å æœ€å¤§çš„ VAD è¯­éŸ³åŒºé—´ï¼Œ
        ç”¨è¯¥åŒºé—´çš„èµ·æ­¢æ›¿ä»£ ASR+æ‰©å±• å¾—åˆ°çš„ç²—è¾¹ç•Œã€‚
        """
        if not vad_timestamps:
            return cut_start, cut_end
        
        # æŸ¥æ‰¾ä¸å½“å‰åˆ‡ç‰‡é‡å çš„ VAD åŒºé—´
        best_overlap = 0
        best_vad = None
        
        for vad in vad_timestamps:
            # è®¡ç®—é‡å 
            overlap_start = max(cut_start - search_margin, vad['start'])
            overlap_end = min(cut_end + search_margin, vad['end'])
            overlap = max(0, overlap_end - overlap_start)
            
            if overlap > best_overlap:
                best_overlap = overlap
                best_vad = vad
        
        if best_vad is None:
            return cut_start, cut_end
        
        # å¦‚æœ VAD åŒºé—´çš„è¾¹ç•Œåœ¨ ASR è¾¹ç•Œé™„è¿‘ï¼Œä½¿ç”¨ VAD çš„ç²¾ç¡®è¾¹ç•Œ
        vad_start = best_vad['start']
        vad_end = best_vad['end']
        
        # cut_start: å– VAD start ä½†ä¸èƒ½æ¯”åŸå§‹ cut_start æ™šå¤ªå¤šï¼ˆé¿å…åˆ‡æ‰æ°”å£ï¼‰
        if abs(vad_start - cut_start) < search_margin:
            cut_start = vad_start
        
        # cut_end: å– VAD end ä½†ä¸èƒ½æ¯”åŸå§‹ cut_end è¿œå¤ªå¤šï¼ˆé¿å…åƒä¸‹ä¸€å¥ï¼‰
        if abs(vad_end - cut_end) < search_margin:
            cut_end = vad_end
        
        return cut_start, cut_end

    # ========== èƒ½é‡æ£€æµ‹ï¼ˆFallbackï¼‰ ==========
    def _refine_cut_point(self, wav, sr, time_s, search_radius=0.15, direction="both"):
        """
        åœ¨ time_s é™„è¿‘æ‰¾åˆ°èƒ½é‡æœ€ä½çš„"é™éŸ³å±±è°·"ä½œä¸ºåˆ‡å‰²ä½ç½®ã€‚
        ä½¿ç”¨ 20ms çª—å£ + æ»‘åŠ¨å¹³å‡å¹³æ»‘ï¼Œé¿å…è¢«ç¬æ—¶ä½èƒ½é‡ï¼ˆæ¸…è¾…éŸ³ç­‰ï¼‰æ¬ºéª—ã€‚
        
        direction:
            "before" â€” åªåœ¨ [time_s - radius, time_s] æœç´¢ï¼ˆç”¨äº cut_startï¼‰
            "after"  â€” åªåœ¨ [time_s, time_s + radius] æœç´¢
            "both"   â€” åœ¨ Â±radius æœç´¢ï¼ˆç”¨äº cut_endï¼Œå¯»æ‰¾æœ€è¿‘çš„é™éŸ³è°·ï¼‰
        """
        center = int(time_s * sr)
        radius = int(search_radius * sr)

        if direction == "before":
            start = max(0, center - radius)
            end = center
        elif direction == "after":
            start = center
            end = min(len(wav), center + radius)
        else:
            start = max(0, center - radius)
            end = min(len(wav), center + radius)

        # æœç´¢åŒºé—´å¤ªçŸ­ï¼ˆ<50msï¼‰åˆ™ä¸å¾®è°ƒ
        if end - start < int(sr * 0.05):
            return time_s
        
        segment = wav[start:end]
        
        # 20ms çª—å£ï¼Œ10ms æ­¥é•¿ï¼ˆè·¨è¶Šå¤§éƒ¨åˆ†çŸ­æš‚é—­æ°”åœé¡¿ï¼‰
        window_size = max(1, int(0.02 * sr))
        step_size = max(1, int(0.01 * sr))
        
        n_windows = (len(segment) - window_size) // step_size + 1
        if n_windows < 2:
            return time_s
        
        energies = []
        positions = []
        for i in range(n_windows):
            w = segment[i * step_size : i * step_size + window_size]
            energies.append(np.sqrt(np.mean(w ** 2)))
            positions.append(start + i * step_size + window_size // 2)
        
        # 5-point æ»‘åŠ¨å¹³å‡å¹³æ»‘ï¼ˆæŠŠé”¯é½¿çŠ¶æ¯›åˆºæŠ¹å¹³ï¼Œå¯»æ‰¾å®½é˜”çš„é™éŸ³å¸¦ï¼‰
        kernel_size = min(5, len(energies))
        kernel = np.ones(kernel_size) / kernel_size
        smoothed = np.convolve(energies, kernel, mode='same')
        
        if len(smoothed) == 0:
            return time_s
        
        # æ‰¾å¹³æ»‘åçš„èƒ½é‡æœ€ä½ç‚¹
        min_idx = np.argmin(smoothed)
        return positions[min_idx] / sr

    def _expand_to_midpoints(self, boundaries: list, total_duration: float) -> list:
        """å°†åˆ‡å‰²ç‚¹æ‰©å±•åˆ°ç›¸é‚»å¥å­é—´éš™ä¸­ï¼Œä½†é™åˆ¶æœ€å¤§æ‰©å±•é‡ä»¥é¿å…åƒè¿›ä¸‹ä¸€å¥ã€‚"""
        MAX_EXPAND_START = 0.15  # cut_start å‘å‰æ‰©å±•ï¼šæœ€å¤š 150msï¼ˆä¿ç•™å¸æ°”/èµ·éŸ³ä½™é‡ï¼‰
        MAX_EXPAND_END = 0.10    # cut_end å‘åæ‰©å±•ï¼šæœ€å¤š 100msï¼ˆè¡¥å¿ ASR å°¾éƒ¨æ—¶é—´æˆ³æ—©é€€ï¼‰

        if len(boundaries) <= 1:
            if boundaries:
                boundaries[0]["cut_start"] = 0.0
                boundaries[0]["cut_end"] = total_duration
            return boundaries

        for i in range(len(boundaries)):
            if i == 0:
                boundaries[i]["cut_start"] = 0.0
            else:
                # ä¸å‰ä¸€å¥çš„é—´éš™ä¸­ç‚¹ï¼Œä½†ä¸è¶…è¿‡ MAX_EXPAND_START
                gap_mid = (boundaries[i - 1]["end"] + boundaries[i]["start"]) / 2
                boundaries[i]["cut_start"] = round(
                    max(gap_mid, boundaries[i]["start"] - MAX_EXPAND_START), 3)

            if i == len(boundaries) - 1:
                boundaries[i]["cut_end"] = total_duration
            else:
                # ä¸åä¸€å¥çš„é—´éš™ä¸­ç‚¹ï¼Œä½†ä¸è¶…è¿‡ MAX_EXPAND_END
                gap_mid = (boundaries[i]["end"] + boundaries[i + 1]["start"]) / 2
                boundaries[i]["cut_end"] = round(
                    min(gap_mid, boundaries[i]["end"] + MAX_EXPAND_END), 3)

        return boundaries

    def stitch(self, split_map, audio_A, audio_B, asr_A, asr_B,
               gap_duration=0.25, padding=0.10, fade_ms=30, use_vad=False, use_forced_align=False):
        log = f"[{self.NODE_NAME}]"

        # è§£æ split_map
        try:
            map_items = json.loads(split_map)
        except json.JSONDecodeError as e:
            print(f"{log} split_map JSON è§£æå¤±è´¥: {e}")
            return (audio_A, "[]")

        # æå–éŸ³é¢‘æ•°æ®
        wav_A, sr_A = self._audio_to_numpy(audio_A)
        wav_B, sr_B = self._audio_to_numpy(audio_B)
        duration_A = len(wav_A) / sr_A
        duration_B = len(wav_B) / sr_B

        # ä½¿ç”¨ç»Ÿä¸€é‡‡æ ·ç‡
        sr = sr_A
        if sr_A != sr_B:
            print(f"{log} è­¦å‘Š: sr_A={sr_A} != sr_B={sr_B}, ä½¿ç”¨ sr_A")

        # Forced Alignment æ¨¡å¼ï¼šå¯¹æ¯ä¸ªè¯´è¯äººåšå­—çº§å¼ºåˆ¶å¯¹é½
        fa_results_A = None
        fa_results_B = None
        if use_forced_align:
            fa_model, fa_tokenizer, fa_aligner = self._load_fa_model()
            if fa_model is not None:
                print(f"{log} ä½¿ç”¨ MMS Forced Alignment å­—çº§å¯¹é½...")
            else:
                print(f"{log} FA æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå›é€€")
                use_forced_align = False

        # VAD æ¨¡å¼ï¼šæå‰å¯¹æ¯ä¸ªè¯´è¯äººçš„å®Œæ•´éŸ³é¢‘è¿è¡Œ VAD
        # å½“ FA å¯ç”¨æ—¶ï¼Œå¦‚æœ VAD ä¹Ÿå¯ç”¨åˆ™åŒæ—¶è¿è¡Œç”¨äºäº¤å‰éªŒè¯
        vad_timestamps_A = None
        vad_timestamps_B = None
        if use_vad or (use_forced_align and use_vad):
            vad_model, vad_utils = self._load_vad_model()
            if vad_model is not None:
                print(f"{log} ä½¿ç”¨ Silero VAD ç²¾ç¡®è¾¹ç•Œæ£€æµ‹...")
                vad_timestamps_A = self._get_vad_timestamps(wav_A, sr_A, vad_model, vad_utils)
                vad_timestamps_B = self._get_vad_timestamps(wav_B, sr_B, vad_model, vad_utils)
                print(f"{log} VAD æ£€æµ‹åˆ° A={len(vad_timestamps_A)} æ®µè¯­éŸ³, B={len(vad_timestamps_B)} æ®µè¯­éŸ³")
            else:
                print(f"{log} VAD åŠ è½½å¤±è´¥")
                if not use_forced_align:
                    use_vad = False

        print(f"{log} Audio A: {duration_A:.2f}s, Audio B: {duration_B:.2f}s, SR: {sr}")

        # æ”¶é›†æ¯ä¸ªè¯´è¯äººçš„å¥å­åˆ—è¡¨
        sentences_A = [item["text"] for item in map_items if item.get("type") == "speech" and item.get("speaker") == "A"]
        sentences_B = [item["text"] for item in map_items if item.get("type") == "speech" and item.get("speaker") == "B"]

        print(f"{log} å¥å­æ•° - A: {len(sentences_A)}, B: {len(sentences_B)}")

        # ASR å¯¹é½åˆ‡åˆ†
        words_A = asr_A.get("words", []) if isinstance(asr_A, dict) else []
        words_B = asr_B.get("words", []) if isinstance(asr_B, dict) else []

        print(f"{log} ASR è¯æ•° - A: {len(words_A)}, B: {len(words_B)}")

        boundaries_A = self._find_sentence_boundaries(words_A, sentences_A, duration_A)
        boundaries_B = self._find_sentence_boundaries(words_B, sentences_B, duration_B)

        print(f"{log} è¾¹ç•Œæ•° - A: {len(boundaries_A)}, B: {len(boundaries_B)}")

        # FA å¯¹é½ï¼ˆåœ¨ ASR è¾¹ç•Œä¹‹åï¼Œç”¨äºæ›¿ä»£/éªŒè¯ ASR è¾¹ç•Œï¼‰
        if use_forced_align:
            print(f"{log} --- Speaker A FA ---")
            fa_results_A = self._forced_align_sentences(wav_A, sr_A, sentences_A)
            print(f"{log} --- Speaker B FA ---")
            fa_results_B = self._forced_align_sentences(wav_B, sr_B, sentences_B)

        # æŒ‰ split_map é¡ºåºæ‹¼æ¥
        audio_segments = []
        segments_info = []
        current_time = 0.0
        idx_A = 0
        idx_B = 0
        prev_speaker = None
        # è·Ÿè¸ªæ¯ä¸ªè¯´è¯äººä¸Šä¸€ä¸ªç‰‡æ®µçš„å®é™… cut_endï¼Œé˜²æ­¢ padding å¯¼è‡´é‡å 
        prev_cut_end = {"A": 0.0, "B": 0.0}

        for item in map_items:
            if item.get("type") == "pause":
                # æ˜¾å¼æš‚åœ
                pause_dur = item.get("duration", 0.3)
                pause_samples = int(pause_dur * sr)
                audio_segments.append(np.zeros(pause_samples, dtype=np.float32))
                current_time += pause_dur
                continue

            if item.get("type") != "speech":
                continue

            speaker = item["speaker"]
            
            # è¯´è¯äººåˆ‡æ¢æ—¶æ’å…¥è¿‡æ¸¡é—´éš™ï¼ˆå¸¦ä½é¢‘å™ªå£°åº•å™ªï¼Œé¿å…æ­»å¯‚ï¼‰
            if prev_speaker is not None and speaker != prev_speaker:
                gap_samples = int(gap_duration * sr)
                # ç”Ÿæˆæä½éŸ³é‡çš„ç²‰å™ªå£°ä»£æ›¿çº¯é™éŸ³ï¼Œå¬æ„Ÿæ›´è‡ªç„¶
                noise = np.random.randn(gap_samples).astype(np.float32) * 0.0003
                audio_segments.append(noise)
                current_time += gap_duration

            # è·å–å¯¹åº”çš„è¾¹ç•Œå’ŒéŸ³é¢‘
            if speaker == "A":
                if idx_A >= len(boundaries_A):
                    print(f"{log} è­¦å‘Š: A çš„å¥å­ç´¢å¼• {idx_A} è¶…å‡ºè¾¹ç•Œæ•° {len(boundaries_A)}")
                    idx_A += 1
                    continue
                boundary = boundaries_A[idx_A]
                wav = wav_A
                idx_A += 1
            elif speaker == "B":
                if idx_B >= len(boundaries_B):
                    print(f"{log} è­¦å‘Š: B çš„å¥å­ç´¢å¼• {idx_B} è¶…å‡ºè¾¹ç•Œæ•° {len(boundaries_B)}")
                    idx_B += 1
                    continue
                boundary = boundaries_B[idx_B]
                wav = wav_B
                idx_B += 1
            else:
                continue

            # åˆ‡å‰²éŸ³é¢‘ç‰‡æ®µï¼ˆä½¿ç”¨ cut_start/cut_endï¼Œå¸¦ paddingï¼‰
            cut_start = boundary.get("cut_start", boundary["start"])
            cut_end = boundary.get("cut_end", boundary["end"])

            # è·å–å½“å‰å¥å­åœ¨è¯´è¯äººå†…çš„ç´¢å¼•
            sent_local_idx = (idx_A - 1) if speaker == "A" else (idx_B - 1)

            # è¾¹ç•Œå¾®è°ƒï¼šFA > VAD > Energy
            if use_forced_align:
                fa_results = fa_results_A if speaker == "A" else fa_results_B
                fa_entry = None
                if fa_results and sent_local_idx < len(fa_results):
                    fa_entry = fa_results[sent_local_idx]
                
                if fa_entry:
                    fa_start, fa_end = fa_entry['start'], fa_entry['end']
                    # æ··åˆç­–ç•¥ï¼šFA ç²¾ç¡®èµ·ç‚¹ + èƒ½é‡æ£€æµ‹è‡ªç„¶æ”¶å°¾
                    cut_start = fa_start
                    cut_end = self._refine_cut_point(wav, sr, fa_end,
                        search_radius=0.15, direction="after")
                    
                    # äº¤å‰éªŒè¯ï¼šåŒæ—¶è®¡ç®— VAD å’Œ Energy çš„ç»“æœåšå¯¹æ¯”
                    if use_vad and vad_timestamps_A is not None:
                        vad_ts = vad_timestamps_A if speaker == "A" else vad_timestamps_B
                        vad_start, vad_end = self._refine_with_vad(
                            boundary.get("cut_start", boundary["start"]),
                            boundary.get("cut_end", boundary["end"]),
                            vad_ts)
                        energy_start = self._refine_cut_point(wav, sr,
                            boundary.get("cut_start", boundary["start"]),
                            search_radius=0.15, direction="before")
                        energy_end = self._refine_cut_point(wav, sr,
                            boundary.get("cut_end", boundary["end"]),
                            search_radius=0.10, direction="both")
                        
                        iou_fa_vad = self._compute_iou(fa_start, fa_end, vad_start, vad_end)
                        iou_fa_energy = self._compute_iou(fa_start, fa_end, energy_start, energy_end)
                        iou_vad_energy = self._compute_iou(vad_start, vad_end, energy_start, energy_end)
                        
                        print(f"{log} ğŸ”¬ {speaker}[{sent_local_idx}] "
                              f"FA=[{fa_start:.3f},{fa_end:.3f}] "
                              f"VAD=[{vad_start:.3f},{vad_end:.3f}] "
                              f"Energy=[{energy_start:.3f},{energy_end:.3f}] | "
                              f"FA-VAD={iou_fa_vad:.3f} FA-Energy={iou_fa_energy:.3f} VAD-Energy={iou_vad_energy:.3f}")
                else:
                    # FA ç»“æœç¼ºå¤±ï¼Œå›é€€åˆ° VAD æˆ– Energy
                    if use_vad and vad_timestamps_A is not None:
                        vad_ts = vad_timestamps_A if speaker == "A" else vad_timestamps_B
                        cut_start, cut_end = self._refine_with_vad(cut_start, cut_end, vad_ts)
                    else:
                        cut_start = self._refine_cut_point(wav, sr, cut_start, search_radius=0.15, direction="before")
                        cut_end = self._refine_cut_point(wav, sr, cut_end, search_radius=0.10, direction="both")
            elif use_vad and vad_timestamps_A is not None:
                vad_ts = vad_timestamps_A if speaker == "A" else vad_timestamps_B
                cut_start, cut_end = self._refine_with_vad(cut_start, cut_end, vad_ts)
            else:
                cut_start = self._refine_cut_point(wav, sr, cut_start, search_radius=0.15, direction="before")
                cut_end = self._refine_cut_point(wav, sr, cut_end, search_radius=0.10, direction="both")

            # åº”ç”¨ paddingï¼šcut_start å…¨é¢ä¿ç•™èµ·éŸ³ä½™é‡ï¼Œcut_end å°‘é‡ä¿ç•™å°¾éŸ³è¡°å‡
            cut_start = max(0, cut_start - padding)
            cut_end = min(len(wav) / sr, cut_end + padding * 0.3)

            # é˜²é‡å ï¼šç¡®ä¿ cut_start ä¸æ—©äºåŒä¸€è¯´è¯äººä¸Šä¸€ä¸ªç‰‡æ®µçš„ cut_end
            if cut_start < prev_cut_end[speaker]:
                cut_start = prev_cut_end[speaker]
            prev_cut_end[speaker] = cut_end

            start_sample = int(cut_start * sr)
            end_sample = int(cut_end * sr)
            end_sample = min(end_sample, len(wav))

            segment = wav[start_sample:end_sample]

            if len(segment) == 0:
                print(f"{log} è­¦å‘Š: ç©ºç‰‡æ®µ at {cut_start:.3f}-{cut_end:.3f}s")
                continue

            seg_duration = len(segment) / sr

            # å¯¹ç‰‡æ®µé¦–å°¾æ–½åŠ ä½™å¼¦æ·¡å…¥æ·¡å‡ºï¼Œä½¿æ‹¼æ¥è¿‡æ¸¡å¹³æ»‘è‡ªç„¶
            fade_seconds = fade_ms / 1000.0
            fade_samples = min(int(fade_seconds * sr), len(segment) // 4)
            if fade_samples > 1:
                # ä½™å¼¦æ·¡å…¥æ·¡å‡ºæ¯”çº¿æ€§æ›´å¹³æ»‘â€”â€”èƒ½é‡å˜åŒ–æ›²çº¿æ›´æ¥è¿‘è‡ªç„¶è¡°å‡
                fade_in = (0.5 * (1 - np.cos(np.linspace(0, np.pi, fade_samples)))).astype(np.float32)
                fade_out = (0.5 * (1 + np.cos(np.linspace(0, np.pi, fade_samples)))).astype(np.float32)
                segment = segment.copy()
                segment[:fade_samples] *= fade_in
                segment[-fade_samples:] *= fade_out

            audio_segments.append(segment)

            # è®°å½• segment info
            original_speaker = item.get("original_speaker", speaker)
            segments_info.append({
                "start": round(current_time, 3),
                "end": round(current_time + seg_duration, 3),
                "text": item["text"],
                "speaker": original_speaker,
            })

            current_time += seg_duration
            prev_speaker = speaker

        # æ‹¼æ¥æ‰€æœ‰ç‰‡æ®µ
        if not audio_segments:
            print(f"{log} é”™è¯¯: æ²¡æœ‰ä»»ä½•éŸ³é¢‘ç‰‡æ®µ")
            return (audio_A, "[]")

        final_audio = np.concatenate(audio_segments)
        total_duration = len(final_audio) / sr
        print(f"{log} æ‹¼æ¥å®Œæˆ: {total_duration:.2f}s, {len(segments_info)} ä¸ªè¯­éŸ³æ®µ")

        # è½¬ä¸º ComfyUI AUDIO æ ¼å¼
        audio_tensor = torch.from_numpy(final_audio).unsqueeze(0).unsqueeze(0)  # (1, 1, samples)
        audio_output = {"waveform": audio_tensor, "sample_rate": sr}

        segments_info_json = json.dumps(segments_info, ensure_ascii=False, indent=2)

        return (audio_output, segments_info_json)


# --- ComfyUI èŠ‚ç‚¹æ³¨å†Œ ---
NODE_CLASS_MAPPINGS = {
    "AIIA_Podcast_Stitcher": AIIA_Podcast_Stitcher,
}
NODE_DISPLAY_NAME_MAPPINGS = {
    "AIIA_Podcast_Stitcher": "ğŸ§µ AIIA Podcast Stitcher",
}
