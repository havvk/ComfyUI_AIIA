
import json
import re

AIIA_EMOTION_LIST = [
    "None", 
    "Happy (å¼€å¿ƒ)", "Sad (æ‚²ä¼¤)", "Angry (æ„¤æ€’)", "Excited (å…´å¥‹)", 
    "Gentle (æ¸©æŸ”)", "Fearful (ææƒ§)", "Surprised (æƒŠè®¶)", "Disappointed (å¤±æœ›)", 
    "Proud (éª„å‚²)", "Anxious (ç„¦è™‘)", "Calm (å†·é™)", "Neutral (ä¸­æ€§)",
    "Affectionate (æ·±æƒ…)", "Awkward (å°´å°¬)", "Determined (åšå®š)", "Hesitant (çŠ¹è±«)",
    "With a hint of shyness (å¸¦ç‚¹ç¾æ¶©)", 
    "With a hint of a smile (å¸¦æœ‰ä¸€ä¸ç¬‘æ„)",
    "Seductive tone (å……æ»¡è¯±æƒ‘åŠ›)", 
    "Crying tone (å¸¦ç€å“­è…”)", 
    "Cheerful tone (å……æ»¡ç¬‘æ„)", 
    "Serious tone (è¯­æ°”ä¸¥è‚ƒ)", 
    "Sarcastic tone (å†·å˜²çƒ­è®½)", 
    "Arrogant tone (è¯­æ°”å‚²æ…¢)", 
    "Cold tone (è¯­æ°”å†·æ·¡)", 
    "Affectionate tone (å……æ»¡çˆ±æ„)", 
    "Whispering (è½»å£°è€³è¯­)", 
    "Shouting (å¤§å£°å«å–Š)", 
    "Rapid fire (è¯­é€Ÿè¾ƒå¿«)", 
    "Slow and deliberate (è¯­é€Ÿè¾ƒæ…¢)", 
    "Tired (ç–²æƒ«ä¸å ª)", 
    "Sleepy tone (ç¡æ„æœ¦èƒ§)", 
    "Drunken tone (é†‰æ„å¾®é†º)", 
    "Professional tone (ä¸“ä¸šæ’­éŸ³)",
    "Magnetic tone (ç£æ€§å—“éŸ³)", 
    "Breathless (æ°”å–˜åå)", 
    "Terrified (æƒŠæä¸‡åˆ†)",
    "Nervous (ç´§å¼ ä¸å®‰)", 
    "Mysterious (è¯­æ°”ç¥ç§˜)", 
    "Enthusiastic (çƒ­æƒ…é«˜æ¶¨)",
    "Lazy tone (è¯­æ°”æ…µæ‡’)", 
    "Gossip tone (å…«å¦è¯­æ°”)", 
    "Innocent (è¯­æ°”å¤©çœŸ)"
]

AIIA_DIALECT_LIST = [
    "None", 
    "Mandarin (æ™®é€šè¯)", "Cantonese (ç²¤è¯­)", "Shanghainese (ä¸Šæµ·è¯)", 
    "Sichuanese (å››å·è¯)", "Northeastern (ä¸œåŒ—è¯)", "Hokkien (é—½å—è¯)", 
    "Hakka (å®¢å®¶è¯)", "Tianjinese (å¤©æ´¥è¯)", "Shandongnese (å±±ä¸œè¯)",
    "Henan (æ²³å—è¯)", "Shaanxi (é™•è¥¿è¯)", "Hunan (æ¹–å—è¯)", "Jiangxi (æ±Ÿè¥¿è¯)",
    "Hubei (æ¹–åŒ—è¯)", "Guizhou (è´µå·è¯)", "Yunnan (äº‘å—è¯)", "Gansu (ç”˜è‚ƒè¯)", "Ningxia (å®å¤è¯)"
]

class AIIA_Podcast_Script_Parser:
    def __init__(self):
        pass

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "script_text": ("STRING", {
                    "multiline": True, 
                    "default": "A: å¤§å®¶å¥½ï¼Œæ¬¢è¿æ”¶å¬æˆ‘ä»¬çš„æ’­å®¢ã€‚\nB: æ˜¯çš„ï¼Œä»Šå¤©æˆ‘ä»¬è¦èŠä¸€ä¸ªå¾ˆæœ‰è¶£çš„è¯é¢˜ã€‚\n(Pause 0.5)\nA: [å¼€å¿ƒ] æ²¡é”™ï¼Œå°±æ˜¯å…³äº AI çš„æœªæ¥ï¼",
                    "placeholder": "è¾“å…¥å‰§æœ¬ï¼Œæ ¼å¼ï¼š\nè§’è‰²å: å°è¯å…§å®¹\n(Pause ç§’æ•°)\n[æƒ…æ„Ÿ] å°è¯"
                }),
            },
            "optional": {
                "speaker_mapping": ("STRING", {
                    "multiline": True, 
                    "default": "A=Speaker_A\nB=Speaker_B",
                    "placeholder": "è§’è‰²æ˜ å°„ (å¯é€‰):\nA=Speaker_01\nB=Speaker_02"
                }),
            }
        }

    RETURN_TYPES = ("STRING", "STRING", "STRING")
    RETURN_NAMES = ("dialogue_json", "speaker_list", "full_script_json")
    FUNCTION = "parse_script"
    CATEGORY = "AIIA/Podcast"

    def parse_script(self, script_text, speaker_mapping=""):
        lines = script_text.strip().split('\n')
        dialogue = []
        speakers = set()
        
        # è§£ææ˜ å°„
        mapping = {}
        if speaker_mapping:
            for line in speaker_mapping.strip().split('\n'):
                if '=' in line:
                    k, v = line.split('=', 1)
                    mapping[k.strip()] = v.strip()

        current_speaker = None
        current_visual = None
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # Filter comments
            if line.startswith("#"):
                continue

            # æ£€æµ‹æš‚åœ (Pause 0.5)
            pause_match = re.match(r'^\((Pause|Wait)\s*(\d*\.?\d+)\s*s?\)$', line, re.IGNORECASE)
            if pause_match:
                duration = float(pause_match.group(2))
                dialogue.append({
                    "type": "pause",
                    "duration": duration
                })
                continue

            # æ£€æµ‹ Visual æ ‡ç­¾ (Visual: https://...)
            # æ”¯æŒ: (Visual: ./img.png) or (Visual: https://example.com)
            visual_match = re.match(r'^\(Visual:\s*(.+?)\)$', line, re.IGNORECASE)
            if visual_match:
                current_visual = visual_match.group(1).strip()
                continue
            
            # æ£€æµ‹æ™®é€šå°è¯ Speaker: Text
            # æ”¯æŒä¸­æ–‡å†’å·å’Œè‹±æ–‡å†’å·
            parts = re.split(r'[:ï¼š]', line, 1)
            if len(parts) == 2:
                speaker_raw = parts[0].strip()
                content = parts[1].strip()
                
                # åº”ç”¨æ˜ å°„
                speaker_id = mapping.get(speaker_raw, speaker_raw)
                speakers.add(speaker_id)
                current_speaker = speaker_id
                
                # è§£ææƒ…æ„Ÿ [Happy] Text
                emotion = None
                emotion_match = re.match(r'^\[(.*?)\]\s*(.*)', content)
                if emotion_match:
                    emotion = emotion_match.group(1)
                    content = emotion_match.group(2)
                
                item = {
                    "type": "speech",
                    "speaker": speaker_id,
                    "text": content,
                    "emotion": emotion
                }
                
                # å¦‚æœæœ‰æš‚å­˜çš„ Visual æ ‡ç­¾ï¼Œé™„åŠ åˆ°è¿™å¥è¯
                if current_visual:
                    item["visual"] = current_visual
                    current_visual = None
                
                dialogue.append(item)
            else:
                # å¯èƒ½æ˜¯å»¶ç»­ä¸Šä¸€å¥çš„å†…å®¹ï¼Œæˆ–è€…æ— æ³•è§£æ
                # ç®€å•èµ·è§ï¼Œå¦‚æœå»¶ç»­ä¸Šä¸€å¥ï¼Œæˆ‘ä»¬è¿½åŠ åˆ°ä¸Šä¸€å¥
                # ä½†æ›´å®‰å…¨çš„åšæ³•æ˜¯å¿½ç•¥æˆ–ä½œä¸ºæ–°çš„ä¸€å¥ï¼ˆä½†è¿™éœ€è¦speakerï¼‰
                # è¿™é‡Œå‡è®¾æ ¼å¼å¿…é¡»ä¸¥æ ¼
                pass

        speaker_list = sorted(list(speakers))
        
        # Build clean dialogue for TTS (without Visual tags) to enable caching
        clean_dialogue = []
        for item in dialogue:
            clean_item = item.copy()
            if "visual" in clean_item:
                del clean_item["visual"]
            clean_dialogue.append(clean_item)

        return (json.dumps(clean_dialogue, ensure_ascii=False, indent=2), ",".join(speaker_list), json.dumps(dialogue, ensure_ascii=False, indent=2))

class AIIA_Segment_Merge:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "segments_info": ("STRING", {"forceInput": True}),
                "full_script": ("STRING", {"forceInput": True}),
            }
        }
    
    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("merged_segments",)
    FUNCTION = "merge_visuals"
    CATEGORY = "AIIA/Utils"

    def merge_visuals(self, segments_info, full_script):
        try:
            segments = json.loads(segments_info)
            script_items = json.loads(full_script)
            
            # Filter script items to only speech to match segments 1:1
            speech_items = [item for item in script_items if item.get("type") == "speech"]
            
            if len(segments) != len(speech_items):
                print(f"[AIIA Merge] Warning: Segment count ({len(segments)}) does not match script speech count ({len(speech_items)}). Visual tags might be misaligned.")
                limit = min(len(segments), len(speech_items))
            else:
                limit = len(segments)
            
            for i in range(limit):
                seg = segments[i]
                script = speech_items[i]
                
                # Check consistency
                # if seg["text"] != script["text"]: ... (Optional check)

                # Copy visual tag if present
                if "visual" in script:
                    seg["visual"] = script["visual"]
            
            return (json.dumps(segments, ensure_ascii=False, indent=2),)

        except Exception as e:
            print(f"[AIIA Merge] Error: {e}")
            return (segments_info,) # Fallback to original


class AIIA_Dialogue_TTS:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "dialogue_json": ("STRING", {"multiline": True}),
                "tts_engine": (["CosyVoice", "VibeVoice", "Qwen3-TTS"], {"default": "CosyVoice"}),
                "pause_duration": ("FLOAT", {"default": 0.5, "min": 0.0, "max": 5.0, "step": 0.1}),
                "speed_global": ("FLOAT", {"default": 1.0, "min": 0.5, "max": 2.0}),
                "batch_mode": (["Natural (Hybrid)", "Strict (Per-Speaker)", "Whole (Single Batch)"], {"default": "Natural (Hybrid)"}),
            },
            "optional": {
                # Speaker A
                "speaker_A_ref": ("AUDIO",),
                "speaker_A_id": ("STRING", {"default": "", "placeholder": "CosyVoice Internal ID (Optional)"}),
                "speaker_A_emotion": (AIIA_EMOTION_LIST, {"default": "None"}),
                "speaker_A_dialect": (AIIA_DIALECT_LIST, {"default": "None"}),
                
                # Speaker B
                "speaker_B_ref": ("AUDIO",),
                "speaker_B_id": ("STRING", {"default": "", "placeholder": "CosyVoice Internal ID (Optional)"}),
                "speaker_B_emotion": (AIIA_EMOTION_LIST, {"default": "None"}),
                "speaker_B_dialect": (AIIA_DIALECT_LIST, {"default": "None"}),

                # Speaker C
                "speaker_C_ref": ("AUDIO",),
                "speaker_C_id": ("STRING", {"default": "", "placeholder": "CosyVoice Internal ID (Optional)"}),
                "speaker_C_emotion": (AIIA_EMOTION_LIST, {"default": "None"}),
                "speaker_C_dialect": (AIIA_DIALECT_LIST, {"default": "None"}),

                # Model Slots and Params (Appended to prevent shift)
                "cosyvoice_model": ("COSYVOICE_MODEL",),
                "vibevoice_model": ("VIBEVOICE_MODEL",),
                "qwen_model": ("QWEN_MODEL",),
                "max_batch_char": ("INT", {"default": 1000, "min": 100, "max": 32768}),
                "cfg_scale": ("FLOAT", {"default": 1.5, "min": 1.0, "max": 10.0, "step": 0.1}),
                "temperature": ("FLOAT", {"default": 0.8, "min": 0.1, "max": 2.0}),
                "top_k": ("INT", {"default": 20, "min": 0, "max": 100}),
                "top_p": ("FLOAT", {"default": 0.95, "min": 0.0, "max": 1.0, "step": 0.05}),
            }
        }
    
    RETURN_TYPES = ("AUDIO", "STRING")
    RETURN_NAMES = ("full_audio", "segments_info")
    FUNCTION = "process_dialogue"
    CATEGORY = "AIIA/Podcast"

    def _load_fallback_audio(self, target="Male"):
        import os
        import torchaudio
        
        # å®šä½ assets ç›®å½•
        current_dir = os.path.dirname(os.path.abspath(__file__))
        assets_dir = os.path.join(current_dir, "assets")
        
        # æ˜ å°„
        filename_map = {
            "Male_HQ": "seed_male_hq.wav",
            "Female_HQ": "seed_female_hq.wav",
            "Male": "seed_male.wav",
            "Female": "seed_female.wav"
        }
        
        filename = filename_map.get(target, "seed_female_hq.wav")
        path = os.path.join(assets_dir, filename)
            
        if not os.path.exists(path):
            print(f"[AIIA Warning] Fallback seed not found at {path}")
            return None
            
        try:
            waveform, sample_rate = torchaudio.load(path)
            # ç»Ÿä¸€è½¬ mono
            if waveform.shape[0] > 1:
                waveform = torch.mean(waveform, dim=0, keepdim=True)
            
            # AIIA Fix: Attenuate volume to prevent clipping/breaking in generation
            waveform = waveform * 0.8
            
            return {"waveform": waveform, "sample_rate": sample_rate}
        except Exception as e:
            print(f"[AIIA Error] Failed to load fallback audio: {e}")
            return None

    def _generate_qwen_batch(self, batch_data, qwen_gen, current_full_wav, sr_ptr, segments_info, time_ptr, speed_global, cfg_scale, temperature, top_k, top_p):
        # This helper processes a batch of Qwen items that are compatible (same routed model, etc.)
        # Qwen's `generate` method takes a single text, so we iterate through the batch.
        for i, item_params in enumerate(batch_data):
            target_model = item_params["tm"]
            text = item_params["tx"]
            spk_id = item_params["sid"]
            ref_audio = item_params["ref"]
            instruct = item_params["ins"]
            spk_name = item_params["original_speaker"] # Added this to item_params in get_qwen_params
            original_item = item_params["original_item"] # Added this to item_params in get_qwen_params

            print(f"  [Qwen Batch] {spk_name}: {text[:30]}... ({target_model['type']})")
            if instruct:
                print(f"  [Qwen Instruct] {instruct}")
            
            try:
                # Call Qwen TTS with routed model
                res = qwen_gen.generate(
                    qwen_model=target_model,
                    text=text,
                    language="Auto",
                    speaker=spk_id,
                    instruct=instruct,
                    reference_audio=ref_audio,
                    dialect=item_params.get("dialect", "None"),
                    seed=42+i, # Use a seed for reproducibility within the batch
                    speed=speed_global,
                    cfg_scale=cfg_scale,
                    temperature=temperature,
                    top_k=top_k,
                    top_p=top_p
                )
                
                generated = res[0]
                wav = generated["waveform"]
                sr = generated["sample_rate"]
                
                if sr_ptr[0] != sr:
                    if current_full_wav:
                        wav = torchaudio.transforms.Resample(sr, sr_ptr[0])(wav)
                    else:
                        sr_ptr[0] = sr
                
                if wav.ndim == 3: wav = wav.squeeze(0)
                if wav.ndim == 1: wav = wav.unsqueeze(0)
                
                # AIIA Fix: Apply tiny fade-in/out to prevent clicks at boundaries
                fade_len = int(sr * 0.05) # 50ms fade
                if wav.shape[-1] > fade_len * 2:
                    fade_in = torch.linspace(0, 1, fade_len, device=wav.device)
                    fade_out = torch.linspace(1, 0, fade_len, device=wav.device)
                    wav[..., :fade_len] *= fade_in
                    wav[..., -fade_len:] *= fade_out
                
                current_full_wav.append(wav)

                # --- Timestamp Tracking ---
                seg_duration = wav.shape[-1] / sr
                seg_start = time_ptr[0]
                seg_end = seg_start + seg_duration
                
                segments_info.append({
                    "start": round(seg_start, 3),
                    "end": round(seg_end, 3),
                    "text": text,
                    "speaker": spk_name,
                    "visual": original_item.get("visual")
                })
                time_ptr[0] += seg_duration
                
                # Add a small gap between segments within a Qwen batch
                gap = 0.2
                gap_samples = int(gap * sr_ptr[0])
                current_full_wav.append(torch.zeros(1, gap_samples))
                time_ptr[0] += gap
                    
            except Exception as e:
                print(f"[Error] Qwen item generation failed: {e}")
                current_full_wav.append(torch.zeros(1, 24000))
                time_ptr[0] += 1.0


    def process_dialogue(self, dialogue_json, tts_engine, pause_duration, speed_global, batch_mode, **kwargs):
        # Extract optional and model-specific params from kwargs
        max_batch_char = kwargs.get("max_batch_char", 1000)
        cfg_scale = kwargs.get("cfg_scale", 1.5)
        temperature = kwargs.get("temperature", 0.8)
        top_k = kwargs.get("top_k", 20)
        top_p = kwargs.get("top_p", 0.95)
        
        cosyvoice_model = kwargs.get("cosyvoice_model")
        vibevoice_model = kwargs.get("vibevoice_model")
        qwen_model = kwargs.get("qwen_model")
        
        # Robustness: ensure max_batch_char is correctly picked up even if shifted or provided as kwarg
        max_batch_char = kwargs.get("max_batch_char", max_batch_char)
        import json
        import torch
        import os
        import torchaudio
        
        # 0. éªŒè¯è¾“å…¥
        if tts_engine == "CosyVoice" and cosyvoice_model is None:
            raise ValueError("é€‰æ‹© CosyVoice å¼•æ“æ—¶ï¼Œå¿…é¡»è¿æ¥ 'cosyvoice_model'ï¼")
        if tts_engine == "VibeVoice" and vibevoice_model is None:
            raise ValueError("é€‰æ‹© VibeVoice å¼•æ“æ—¶ï¼Œå¿…é¡»è¿æ¥ 'vibevoice_model'ï¼")
        if tts_engine == "Qwen3-TTS":
            if qwen_model is None:
                raise ValueError("é€‰æ‹© Qwen3-TTS å¼•æ“æ—¶ï¼Œå¿…é¡»è¿æ¥ 'qwen_model'ï¼(å¦‚æœéœ€è¦å¤šä¸ªæ¨¡å‹ï¼Œè¯·ä½¿ç”¨ Router èŠ‚ç‚¹æ‰“åŒ…)")

        dialogue = json.loads(dialogue_json)
        full_waveform = []
        sample_rate = 22050 
        
        from .aiia_cosyvoice_nodes import AIIA_CosyVoice_TTS
        from .aiia_vibevoice_nodes import AIIA_VibeVoice_TTS
        from .aiia_qwen_nodes import AIIA_Qwen_TTS
        
        cosy_gen = AIIA_CosyVoice_TTS()
        vibe_gen = AIIA_VibeVoice_TTS()
        qwen_gen = AIIA_Qwen_TTS()

        print(f"[AIIA Podcast] å¼€å§‹å¤„ç†å¯¹è¯ï¼Œå…± {len(dialogue)} ä¸ªç‰‡æ®µã€‚å¼•æ“: {tts_engine}")

        # --- Helper: è§£æçœŸå® Speaker Key ---
        def get_speaker_key(speaker_name):
            spk_key = speaker_name.strip()
            if spk_key.upper() in ["A", "B", "C"]: return spk_key.upper()
            clean = re.sub(r'speaker[ _-]*', '', spk_key, flags=re.IGNORECASE).strip()
            if clean and clean[0].upper() in ["A", "B", "C"]: return clean[0].upper()
            if spk_key[-1].upper() in ["A", "B", "C"]: return spk_key[-1].upper()
            return spk_key[0].upper()

        # --- Helper: è·å–å‚è€ƒéŸ³é¢‘ ---
        def get_ref_audio(spk_key):
            ref = kwargs.get(f"speaker_{spk_key}_ref")
            if ref is not None: return ref
            
            # Fallback logic
            fallback_target = "Male"
            if "A" in spk_key: fallback_target = "Male_HQ"
            elif "B" in spk_key: fallback_target = "Female_HQ"
            elif "C" in spk_key: fallback_target = "Male"
            else: fallback_target = "Female"
            
            print(f"  [Auto-Fallback] Speaker {spk_key} using {fallback_target}")
            return self._load_fallback_audio(fallback_target)

        # --- Segment Tracker ---
        segments_info = [] # List of {"start": float, "end": float, "text": str, "speaker": str}
        time_ptr = [0.0] # Mutable time pointer

        # --- Batch Flusher ---
        def flush_batch(batch_items, current_full_wav, sr_ptr):
            if not batch_items: return
            
            if tts_engine == "VibeVoice":
                # VibeVoice Hybrid Batching
                unique_speakers = {} 
                ref_audio_list = []
                final_text_lines = []
                
                # Pre-calculate total text length for interpolation
                total_char_len = 0
                item_lengths = []

                for item in batch_items:
                    spk_name = item["speaker"]
                    spk_key = get_speaker_key(spk_name)
                    
                    if spk_key not in unique_speakers:
                        unique_speakers[spk_key] = len(unique_speakers)
                        ref_audio_list.append(get_ref_audio(spk_key))
                    
                    internal_id = unique_speakers[spk_key]
                    text = item["text"]
                    
                    # VibeVoice does not support emotion macro text tags.
                    # We send only pure text to prevent the model from reading tags aloud.
                    
                    # ç¡®ä¿æ¯å¥ä»¥å¥æœ«æ ‡ç‚¹ç»“å°¾ï¼Œå¦åˆ™ TTS è¯­è°ƒä¸ä¼šè‡ªç„¶æ”¶æŸ
                    text_stripped = text.rstrip() if text else ""
                    if text_stripped and text_stripped[-1] not in 'ã€‚ï¼ï¼Ÿ!?.â€¦':
                        text = text_stripped + 'ã€‚'
                    
                    char_len = len(text) if text else 1
                    total_char_len += char_len
                    item_lengths.append(char_len)

                    final_text_lines.append(f"[{internal_id + 1}]: {text}")
                    print(f"    ğŸ“ å¥ {len(final_text_lines)}: speaker='{spk_name}' â†’ key='{spk_key}' â†’ id=[{internal_id + 1}] | '{text[:30]}...'")
                
                # ç”¨åŒæ¢è¡Œåˆ†éš”ï¼Œå¼ºåˆ¶ TTS åœ¨å¥é—´äº§ç”Ÿè‡ªç„¶åœé¡¿
                full_text = "\n\n".join(final_text_lines)
                print(f"  [Batch Process] Processing {len(batch_items)} segments using {len(unique_speakers)} speakers.")
                print(f"  [Speaker Map] {unique_speakers}")
                ref_info = [f"sr={r.get('sample_rate','?')}, shape={r['waveform'].shape}" if r else "None" for r in ref_audio_list]
                print(f"  [Ref Audio] {len(ref_audio_list)} items: {ref_info}")
                print(f"  [VibeVoice Input Text]\n{'='*60}\n{full_text}\n{'='*60}")
                
                try:
                    res = vibe_gen.generate(
                        vibevoice_model=vibevoice_model,
                        text=full_text,
                        voice_preset="Female_HQ",
                        reference_audio=ref_audio_list,
                        cfg_scale=cfg_scale,
                        ddpm_steps=20,
                        speed=speed_global,
                        normalize_text=True,
                        do_sample="auto",
                        temperature=temperature,
                        top_k=top_k,
                        top_p=top_p
                    )
                    
                    if res and res[0]:
                        wav = res[0]["waveform"]
                        sr = res[0]["sample_rate"]
                        
                        if sr_ptr[0] != sr:
                            if current_full_wav:
                                wav = torchaudio.transforms.Resample(sr, sr_ptr[0])(wav)
                            else:
                                sr_ptr[0] = sr
                        
                        if wav.ndim == 3: wav = wav.squeeze(0)
                        if wav.ndim == 1: wav = wav.unsqueeze(0)
                        current_full_wav.append(wav)

                        # --- Timestamp Interpolation ---
                        total_duration = wav.shape[-1] / sr
                        current_batch_start = time_ptr[0]
                        
                        accumulated_time = 0.0
                        for idx, item in enumerate(batch_items):
                            item_len = item_lengths[idx]
                            fraction = item_len / max(total_char_len, 1)
                            est_duration = fraction * total_duration
                            
                            seg_start = current_batch_start + accumulated_time
                            seg_end = seg_start + est_duration
                            
                            segments_info.append({
                                "start": round(seg_start, 3),
                                "end": round(seg_end, 3),
                                "text": item["text"],
                                "speaker": item["speaker"],
                                "visual": item.get("visual")
                            })
                            accumulated_time += est_duration
                        
                        time_ptr[0] += total_duration

                except Exception as e:
                    print(f"[Error] Batch generation failed: {e}")
                    current_full_wav.append(torch.zeros(1, 24000))
                    # Fallback timestamps
                    start_t = time_ptr[0]
                    end_t = start_t + 1.0
                    for item in batch_items:
                        segments_info.append({
                            "start": start_t, "end": end_t, "text": item["text"] + " (Gen Failed)", "speaker": item["speaker"]
                        })
                    time_ptr[0] += 1.0
                    
            elif tts_engine == "Qwen3-TTS":
                # --- Qwen3-TTS Batch Maximization ---
                current_batch = []
                current_batch_char = 0
                current_hash = None

                # Batching items by "compatibility"
                # Compatibility = Same routed model, speaker_id, and reference_audio
                def get_qwen_params(it):
                    sk = get_speaker_key(it["speaker"])
                    tx = it["text"]
                    em = it.get("emotion", "None")
                    sid = kwargs.get(f"speaker_{sk}_id", "Vivian") # Default to Vivian if empty
                    if not sid.strip(): sid = "Vivian"
                    ref = get_ref_audio(sk)
                    pemf = kwargs.get(f"speaker_{sk}_emotion", "None")
                    dia = kwargs.get(f"speaker_{sk}_dialect", "None")
                    
                    me = em if em and em != "None" else ""
                    if pemf and pemf != "None":
                        el = pemf.split(" (")[0] if " (" in pemf else pemf
                        me = f"{me}ï¼Œ{el}" if me else el
                    ins = f"{me}ã€‚" if me else ""
                    
                    # Routing: Use bundle if available, else check direct slots
                    tm = qwen_model
                    if qwen_model and qwen_model.get("is_bundle"):
                        if ref is not None: tm = qwen_model.get("base") or qwen_model.get("default")
                        elif ins: tm = qwen_model.get("design") or qwen_model.get("default")
                        else: tm = qwen_model.get("custom") or qwen_model.get("default")
                    elif tm is None:
                        # Fallback for deprecated single-slot inputs
                        if ref is not None: tm = qwen_base_model or qwen_custom_model
                        elif ins: tm = qwen_design_model or qwen_custom_model
                        else: tm = qwen_custom_model or qwen_base_model or qwen_design_model
                    
                    # Dialect is part of compatibility
                    return {
                        "tm": tm, "tx": tx, "sid": sid, "ref": ref, "ins": ins, "me": me, "sk": sk,
                        "dialect": dia,
                        "h": (id(tm), dia, ins), # Grouping key â€“ includes emotion instruct
                        "original_speaker": it["speaker"],
                        "original_item": it # Keep original item for visual tag
                    }

                for it in batch_items:
                    p = get_qwen_params(it)
                    
                    # Check if the current item is compatible with the current batch
                    # Compatibility: same routed model (via hash), and total char count within limit
                    can_m = (current_hash is not None and p["h"] == current_hash and (current_batch_char + len(p["tx"]) < max_batch_char))
                    
                    if not can_m:
                        # If not compatible, or if it's the first item, flush the previous batch (if any)
                        if current_batch:
                            self._generate_qwen_batch(current_batch, qwen_gen, current_full_wav, sr_ptr, segments_info, time_ptr, speed_global, cfg_scale, temperature, top_k, top_p)
                        
                        # Start a new batch
                        current_batch = [p]
                        current_batch_char = len(p["tx"])
                        current_hash = p["h"]
                    else:
                        # Add to current batch
                        current_batch.append(p)
                        current_batch_char += len(p["tx"])
                
                # Flush any remaining items in the last batch
                if current_batch:
                    self._generate_qwen_batch(current_batch, qwen_gen, current_full_wav, sr_ptr, segments_info, time_ptr, speed_global, cfg_scale, temperature, top_k, top_p)

            else:
                # CosyVoice (Iterative)
                for i, item in enumerate(batch_items):
                    spk_name = item["speaker"]
                    spk_key = get_speaker_key(spk_name)
                    text = item["text"]
                    emotion = item.get("emotion") # In CosyVoice, we put it in [] in text
                    
                    # Emotion compatibility check
                    is_expressive = False
                    if cosyvoice_model:
                        is_expressive = cosyvoice_model.get("is_instruct") or cosyvoice_model.get("is_v2") or cosyvoice_model.get("is_v3")
                    
                    if is_expressive:
                        # Merge preset emotion
                        preset_emo_full = kwargs.get(f"speaker_{spk_key}_emotion", "None")
                        if preset_emo_full and preset_emo_full != "None":
                            emo_label = preset_emo_full.split(" (")[0] if " (" in preset_emo_full else preset_emo_full
                            if emotion: text = f"[{emotion}, {emo_label}] {text}"
                            else: text = f"[{emo_label}] {text}"
                        elif emotion:
                            text = f"[{emotion}] {text}"

                    ref_audio = get_ref_audio(spk_key)
                    
                    # CosyVoice uses instruct_text for emotion, so we use the merged emotion for it
                    merged_emo_for_instruct = ""
                    if is_expressive:
                        if preset_emo_full and preset_emo_full != "None":
                            merged_emo_for_instruct = preset_emo_full.split(" (")[0] if " (" in preset_emo_full else preset_emo_full
                        elif item.get("emotion") and item.get("emotion") != "None": # Use script emotion if no preset
                            merged_emo_for_instruct = item.get("emotion")

                    instruct = f"{merged_emo_for_instruct}." if merged_emo_for_instruct else ""
                    
                    print(f"  [CosyVoice Text] {spk_name}: {text}")
                    if instruct:
                        print(f"  [CosyVoice Instruct] {instruct}")
                    try:
                        res = cosy_gen.generate(
                            model=cosyvoice_model,
                            tts_text=text,
                            instruct_text=instruct,
                            spk_id="",
                            speed=speed_global,
                            seed=42+i,
                            dialect=kwargs.get(f"speaker_{spk_key}_dialect", "None (Auto)"),
                            emotion="None (Neutral)",
                            reference_audio=ref_audio
                        )
                        generated = res[0]
                        wav = generated["waveform"]
                        sr = generated["sample_rate"]
                        
                        if sr_ptr[0] != sr:
                            if current_full_wav:
                                wav = torchaudio.transforms.Resample(sr, sr_ptr[0])(wav)
                            else:
                                sr_ptr[0] = sr
                        
                        if wav.ndim == 3: wav = wav.squeeze(0)
                        if wav.ndim == 1: wav = wav.unsqueeze(0)
                        current_full_wav.append(wav)

                        # --- Timestamp Tracking ---
                        seg_duration = wav.shape[-1] / sr
                        seg_start = time_ptr[0]
                        seg_end = seg_start + seg_duration
                        
                        segments_info.append({
                            "start": round(seg_start, 3),
                            "end": round(seg_end, 3),
                            "text": text,
                            "speaker": spk_name,
                            "visual": item.get("visual")
                        })
                        time_ptr[0] += seg_duration
                        
                        if tts_engine == "CosyVoice" and i < len(batch_items) - 1:
                            gap = 0.2
                            gap_samples = int(gap * sr_ptr[0])
                            current_full_wav.append(torch.zeros(1, gap_samples))
                            time_ptr[0] += gap
                            
                    except Exception as e:
                        print(f"[Error] Item generation failed: {e}")
                        current_full_wav.append(torch.zeros(1, 16000))
                        time_ptr[0] += 1.0

                # --- Main Loop ---
        batch_buffer = []
        sr_wrapper = [22050 if tts_engine == "CosyVoice" else 24000] # Mutable ref
        
        for i, item in enumerate(dialogue):
            # Check mode
            batch_mode = kwargs.get("batch_mode", "Natural (Hybrid)")

            if item["type"] == "pause":
                # In Whole Batch mode, we IGNORE pauses to maintain single-batch integrity
                if batch_mode == "Whole (Single Batch)":
                    print("  [Whole Batch] Ignoring explicit pause to maintain flow.")
                    continue

                # Flush current speech batch
                flush_batch(batch_buffer, full_waveform, sr_wrapper)
                batch_buffer = []
                
                # Append Pause
                duration = item.get("duration", pause_duration)
                print(f"  [Pause] {duration}s")
                silence_samples = int(duration * sr_wrapper[0])
                if silence_samples > 0:
                    full_waveform.append(torch.zeros(1, silence_samples))
                    time_ptr[0] += duration # Track pause time

            
            elif item["type"] == "speech":
                # Check for strict mode
                batch_mode = kwargs.get("batch_mode", "Natural (Hybrid)")
                
                # If strict mode, flush on speaker change
                if batch_mode == "Strict (Per-Speaker)" and batch_buffer:
                    last_spk = batch_buffer[-1]["speaker"]
                    curr_spk = item["speaker"]
                    # Simple name check or mapped key check? 
                    # Let's use name check for simplicity as key derivation is same
                    if last_spk != curr_spk:
                        flush_batch(batch_buffer, full_waveform, sr_wrapper)
                        batch_buffer = []

                batch_buffer.append(item)
        
        # Final Flush
        flush_batch(batch_buffer, full_waveform, sr_wrapper)

        # Merge
        sample_rate = sr_wrapper[0]
        if not full_waveform:
            final_tensor = torch.zeros(1, 16000)
            sample_rate = 16000
        else:
            final_tensor = torch.cat(full_waveform, dim=1)
        
        # Output Segments Info JSON
        segments_json = json.dumps(segments_info, ensure_ascii=False, indent=2)

        return ({"waveform": final_tensor.unsqueeze(0), "sample_rate": sample_rate}, segments_json)


# èŠ‚ç‚¹æ˜ å°„å¯¼å‡º
NODE_CLASS_MAPPINGS = {
    "AIIA_Podcast_Script_Parser": AIIA_Podcast_Script_Parser,
    "AIIA_Dialogue_TTS": AIIA_Dialogue_TTS,
    "AIIA_Segment_Merge": AIIA_Segment_Merge
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "AIIA_Podcast_Script_Parser": "AIIA Podcast Script Parser",
    "AIIA_Dialogue_TTS": "AIIA Dialogue TTS (Multi-Role)",
    "AIIA_Segment_Merge": "AIIA Segment Merge (Visual)"
}
