
import json
import re

class AIIA_Podcast_Script_Parser:
    def __init__(self):
        pass

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "script_text": ("STRING", {
                    "multiline": True, 
                    "default": "A: Â§ßÂÆ∂Â•ΩÔºåÊ¨¢ËøéÊî∂Âê¨Êàë‰ª¨ÁöÑÊí≠ÂÆ¢„ÄÇ\nB: ÊòØÁöÑÔºå‰ªäÂ§©Êàë‰ª¨Ë¶ÅËÅä‰∏Ä‰∏™ÂæàÊúâË∂£ÁöÑËØùÈ¢ò„ÄÇ\n(Pause 0.5)\nA: [ÂºÄÂøÉ] Ê≤°ÈîôÔºåÂ∞±ÊòØÂÖ≥‰∫é AI ÁöÑÊú™Êù•ÔºÅ",
                    "placeholder": "ËæìÂÖ•ÂâßÊú¨ÔºåÊ†ºÂºèÔºö\nËßíËâ≤Âêç: Âè∞ËØçÂÖßÂÆπ\n(Pause ÁßíÊï∞)\n[ÊÉÖÊÑü] Âè∞ËØç"
                }),
            },
            "optional": {
                "speaker_mapping": ("STRING", {
                    "multiline": True, 
                    "default": "A=Speaker_A\nB=Speaker_B",
                    "placeholder": "ËßíËâ≤Êò†Â∞Ñ (ÂèØÈÄâ):\nA=Speaker_01\nB=Speaker_02"
                }),
            }
        }

    RETURN_TYPES = ("STRING", "STRING", "STRING")
    RETURN_NAMES = ("dialogue_json", "speaker_list", "full_script_json")
    FUNCTION = "parse_script"
    CATEGORY = "AIIA/Podcast"

    def parse_script(self, script_text, speaker_mapping=""):
        lines = script_text.strip().split('\n')
        dialogue = []
        speakers = set()
        
        # Ëß£ÊûêÊò†Â∞Ñ
        mapping = {}
        if speaker_mapping:
            for line in speaker_mapping.strip().split('\n'):
                if '=' in line:
                    k, v = line.split('=', 1)
                    mapping[k.strip()] = v.strip()

        current_speaker = None
        current_visual = None
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # Filter comments
            if line.startswith("#"):
                continue

            # Ê£ÄÊµãÊöÇÂÅú (Pause 0.5)
            pause_match = re.match(r'^\((Pause|Wait)\s*(\d*\.?\d+)\s*s?\)$', line, re.IGNORECASE)
            if pause_match:
                duration = float(pause_match.group(2))
                dialogue.append({
                    "type": "pause",
                    "duration": duration
                })
                continue

            # Ê£ÄÊµã Visual Ê†áÁ≠æ (Visual: https://...)
            # ÊîØÊåÅ: (Visual: ./img.png) or (Visual: https://example.com)
            visual_match = re.match(r'^\(Visual:\s*(.+?)\)$', line, re.IGNORECASE)
            if visual_match:
                current_visual = visual_match.group(1).strip()
                continue
            
            # Ê£ÄÊµãÊôÆÈÄöÂè∞ËØç Speaker: Text
            # ÊîØÊåÅ‰∏≠ÊñáÂÜíÂè∑ÂíåËã±ÊñáÂÜíÂè∑
            parts = re.split(r'[:Ôºö]', line, 1)
            if len(parts) == 2:
                speaker_raw = parts[0].strip()
                content = parts[1].strip()
                
                # Â∫îÁî®Êò†Â∞Ñ
                speaker_id = mapping.get(speaker_raw, speaker_raw)
                speakers.add(speaker_id)
                current_speaker = speaker_id
                
                # Ëß£ÊûêÊÉÖÊÑü [Happy] Text
                emotion = None
                emotion_match = re.match(r'^\[(.*?)\]\s*(.*)', content)
                if emotion_match:
                    emotion = emotion_match.group(1)
                    content = emotion_match.group(2)
                
                item = {
                    "type": "speech",
                    "speaker": speaker_id,
                    "text": content,
                    "emotion": emotion
                }
                
                # Â¶ÇÊûúÊúâÊöÇÂ≠òÁöÑ Visual Ê†áÁ≠æÔºåÈôÑÂä†Âà∞ËøôÂè•ËØù
                if current_visual:
                    item["visual"] = current_visual
                    current_visual = None
                
                dialogue.append(item)
            else:
                # ÂèØËÉΩÊòØÂª∂Áª≠‰∏ä‰∏ÄÂè•ÁöÑÂÜÖÂÆπÔºåÊàñËÄÖÊó†Ê≥ïËß£Êûê
                # ÁÆÄÂçïËµ∑ËßÅÔºåÂ¶ÇÊûúÂª∂Áª≠‰∏ä‰∏ÄÂè•ÔºåÊàë‰ª¨ËøΩÂä†Âà∞‰∏ä‰∏ÄÂè•
                # ‰ΩÜÊõ¥ÂÆâÂÖ®ÁöÑÂÅöÊ≥ïÊòØÂøΩÁï•Êàñ‰Ωú‰∏∫Êñ∞ÁöÑ‰∏ÄÂè•Ôºà‰ΩÜËøôÈúÄË¶ÅspeakerÔºâ
                # ËøôÈáåÂÅáËÆæÊ†ºÂºèÂøÖÈ°ª‰∏•Ê†º
                pass

        speaker_list = sorted(list(speakers))
        
        # Build clean dialogue for TTS (without Visual tags) to enable caching
        clean_dialogue = []
        for item in dialogue:
            clean_item = item.copy()
            if "visual" in clean_item:
                del clean_item["visual"]
            clean_dialogue.append(clean_item)

        return (json.dumps(clean_dialogue, ensure_ascii=False, indent=2), ",".join(speaker_list), json.dumps(dialogue, ensure_ascii=False, indent=2))

class AIIA_Segment_Merge:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "segments_info": ("STRING", {"forceInput": True}),
                "full_script": ("STRING", {"forceInput": True}),
            }
        }
    
    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("merged_segments",)
    FUNCTION = "merge_visuals"
    CATEGORY = "AIIA/Utils"

    def merge_visuals(self, segments_info, full_script):
        try:
            segments = json.loads(segments_info)
            script_items = json.loads(full_script)
            
            # Filter script items to only speech to match segments 1:1
            speech_items = [item for item in script_items if item.get("type") == "speech"]
            
            if len(segments) != len(speech_items):
                print(f"[AIIA Merge] Warning: Segment count ({len(segments)}) does not match script speech count ({len(speech_items)}). Visual tags might be misaligned.")
                limit = min(len(segments), len(speech_items))
            else:
                limit = len(segments)
            
            for i in range(limit):
                seg = segments[i]
                script = speech_items[i]
                
                # Check consistency
                # if seg["text"] != script["text"]: ... (Optional check)

                # Copy visual tag if present
                if "visual" in script:
                    seg["visual"] = script["visual"]
            
            return (json.dumps(segments, ensure_ascii=False, indent=2),)

        except Exception as e:
            print(f"[AIIA Merge] Error: {e}")
            return (segments_info,) # Fallback to original


class AIIA_Dialogue_TTS:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "dialogue_json": ("STRING", {"forceInput": True}),
                "tts_engine": (["CosyVoice", "VibeVoice"], {"default": "CosyVoice"}),
                "pause_duration": ("FLOAT", {"default": 0.5, "min": 0.0, "max": 5.0, "step": 0.1}),
                "speed_global": ("FLOAT", {"default": 1.0, "min": 0.5, "max": 2.0}),
                "batch_mode": (["Natural (Hybrid)", "Strict (Per-Speaker)", "Whole (Single Batch)"], {"default": "Natural (Hybrid)"}),
                
                # VibeVoice Specific Params
                "cfg_scale": ("FLOAT", {"default": 1.5, "min": 1.0, "max": 10.0, "step": 0.1}),
                "temperature": ("FLOAT", {"default": 0.8, "min": 0.1, "max": 2.0}),
                "top_k": ("INT", {"default": 20, "min": 0, "max": 100}),
                "top_p": ("FLOAT", {"default": 0.95, "min": 0.0, "max": 1.0}),
            },
            "optional": {
                "cosyvoice_model": ("COSYVOICE_MODEL",),
                "vibevoice_model": ("VIBEVOICE_MODEL",),
                
                # Speaker A
                "speaker_A_ref": ("AUDIO",),
                "speaker_A_id": ("STRING", {"default": "", "placeholder": "CosyVoice ÂÜÖÈÉ®Èü≥Ëâ≤ID (ÂèØÈÄâ)"}),
                
                # Speaker B
                "speaker_B_ref": ("AUDIO",),
                "speaker_B_id": ("STRING", {"default": "", "placeholder": "CosyVoice ÂÜÖÈÉ®Èü≥Ëâ≤ID (ÂèØÈÄâ)"}),

                # Speaker C
                "speaker_C_ref": ("AUDIO",),
                "speaker_C_id": ("STRING", {"default": "", "placeholder": "CosyVoice ÂÜÖÈÉ®Èü≥Ëâ≤ID (ÂèØÈÄâ)"}),
            }
        }
    
    RETURN_TYPES = ("AUDIO", "STRING")
    RETURN_NAMES = ("full_audio", "segments_info")
    FUNCTION = "process_dialogue"
    CATEGORY = "AIIA/Podcast"

    def _load_fallback_audio(self, target="Male"):
        import os
        import torchaudio
        
        # ÂÆö‰Ωç assets ÁõÆÂΩï
        current_dir = os.path.dirname(os.path.abspath(__file__))
        assets_dir = os.path.join(current_dir, "assets")
        
        # Êò†Â∞Ñ
        filename_map = {
            "Male_HQ": "seed_male_hq.wav",
            "Female_HQ": "seed_female_hq.wav",
            "Male": "seed_male.wav",
            "Female": "seed_female.wav"
        }
        
        filename = filename_map.get(target, "seed_female_hq.wav")
        path = os.path.join(assets_dir, filename)
            
        if not os.path.exists(path):
            print(f"[AIIA Warning] Fallback seed not found at {path}")
            return None
            
        try:
            waveform, sample_rate = torchaudio.load(path)
            # Áªü‰∏ÄËΩ¨ mono
            if waveform.shape[0] > 1:
                waveform = torch.mean(waveform, dim=0, keepdim=True)
            
            # AIIA Fix: Attenuate volume to prevent clipping/breaking in generation
            waveform = waveform * 0.8
            
            return {"waveform": waveform, "sample_rate": sample_rate}
        except Exception as e:
            print(f"[AIIA Error] Failed to load fallback audio: {e}")
            return None

    def process_dialogue(self, dialogue_json, tts_engine, pause_duration, speed_global, 
                         cosyvoice_model=None, vibevoice_model=None, 
                         cfg_scale=1.5, temperature=0.8, top_k=20, top_p=0.95, **kwargs):
        import json
        import torch
        import os
        import torchaudio
        
        # 0. È™åËØÅËæìÂÖ•
        if tts_engine == "CosyVoice" and cosyvoice_model is None:
            raise ValueError("ÈÄâÊã© CosyVoice ÂºïÊìéÊó∂ÔºåÂøÖÈ°ªËøûÊé• 'cosyvoice_model'ÔºÅ")
        if tts_engine == "VibeVoice" and vibevoice_model is None:
            raise ValueError("ÈÄâÊã© VibeVoice ÂºïÊìéÊó∂ÔºåÂøÖÈ°ªËøûÊé• 'vibevoice_model'ÔºÅ")

        dialogue = json.loads(dialogue_json)
        full_waveform = []
        sample_rate = 22050 
        
        from .aiia_cosyvoice_nodes import AIIA_CosyVoice_TTS
        from .aiia_vibevoice_nodes import AIIA_VibeVoice_TTS
        
        cosy_gen = AIIA_CosyVoice_TTS()
        vibe_gen = AIIA_VibeVoice_TTS()

        print(f"[AIIA Podcast] ÂºÄÂßãÂ§ÑÁêÜÂØπËØùÔºåÂÖ± {len(dialogue)} ‰∏™ÁâáÊÆµ„ÄÇÂºïÊìé: {tts_engine}")

        # --- Helper: Ëß£ÊûêÁúüÂÆû Speaker Key ---
        def get_speaker_key(speaker_name):
            spk_key = speaker_name.strip()
            if spk_key.upper() in ["A", "B", "C"]: return spk_key.upper()
            clean = re.sub(r'speaker[ _-]*', '', spk_key, flags=re.IGNORECASE).strip()
            if clean and clean[0].upper() in ["A", "B", "C"]: return clean[0].upper()
            if spk_key[-1].upper() in ["A", "B", "C"]: return spk_key[-1].upper()
            return spk_key[0].upper()

        # --- Helper: Ëé∑ÂèñÂèÇËÄÉÈü≥È¢ë ---
        def get_ref_audio(spk_key):
            ref = kwargs.get(f"speaker_{spk_key}_ref")
            if ref is not None: return ref
            
            # Fallback logic
            fallback_target = "Male"
            if "A" in spk_key: fallback_target = "Male_HQ"
            elif "B" in spk_key: fallback_target = "Female_HQ"
            elif "C" in spk_key: fallback_target = "Male"
            else: fallback_target = "Female"
            
            print(f"  [Auto-Fallback] Speaker {spk_key} using {fallback_target}")
            return self._load_fallback_audio(fallback_target)

        # --- Segment Tracker ---
        segments_info = [] # List of {"start": float, "end": float, "text": str, "speaker": str}
        time_ptr = [0.0] # Mutable time pointer

        # --- Batch Flusher ---
        def flush_batch(batch_items, current_full_wav, sr_ptr):
            if not batch_items: return
            
            if tts_engine == "VibeVoice":
                # VibeVoice Hybrid Batching
                unique_speakers = {} 
                ref_audio_list = []
                final_text_lines = []
                
                # Pre-calculate total text length for interpolation
                total_char_len = 0
                item_lengths = []

                for item in batch_items:
                    spk_name = item["speaker"]
                    spk_key = get_speaker_key(spk_name)
                    
                    if spk_key not in unique_speakers:
                        unique_speakers[spk_key] = len(unique_speakers)
                        ref_audio_list.append(get_ref_audio(spk_key))
                    
                    internal_id = unique_speakers[spk_key]
                    text = item["text"]
                    
                    # Clean text for length calc (approx)
                    clean_text = re.sub(r'\[.*?\]', '', text).strip()
                    char_len = len(clean_text) if clean_text else 1
                    total_char_len += char_len
                    item_lengths.append(char_len)

                    final_text_lines.append(f"Speaker {internal_id}: {text}")
                
                full_text = "\n".join(final_text_lines)
                print(f"  [Batch Process] Processing {len(batch_items)} segments using {len(unique_speakers)} speakers.")
                
                try:
                    res = vibe_gen.generate(
                        vibevoice_model=vibevoice_model,
                        text=full_text,
                        reference_audio=ref_audio_list,
                        cfg_scale=cfg_scale,
                        ddpm_steps=20,
                        speed=speed_global,
                        normalize_text=True,
                        do_sample="auto",
                        temperature=temperature,
                        top_k=top_k,
                        top_p=top_p
                    )
                    
                    if res and res[0]:
                        wav = res[0]["waveform"]
                        sr = res[0]["sample_rate"]
                        
                        if sr_ptr[0] != sr:
                            if current_full_wav:
                                wav = torchaudio.transforms.Resample(sr, sr_ptr[0])(wav)
                            else:
                                sr_ptr[0] = sr
                        
                        if wav.ndim == 3: wav = wav.squeeze(0)
                        if wav.ndim == 1: wav = wav.unsqueeze(0)
                        current_full_wav.append(wav)

                        # --- Timestamp Interpolation ---
                        total_duration = wav.shape[-1] / sr
                        current_batch_start = time_ptr[0]
                        
                        accumulated_time = 0.0
                        for idx, item in enumerate(batch_items):
                            item_len = item_lengths[idx]
                            fraction = item_len / max(total_char_len, 1)
                            est_duration = fraction * total_duration
                            
                            seg_start = current_batch_start + accumulated_time
                            seg_end = seg_start + est_duration
                            
                            segments_info.append({
                                "start": round(seg_start, 3),
                                "end": round(seg_end, 3),
                                "text": item["text"],
                                "speaker": item["speaker"],
                                "visual": item.get("visual")
                            })
                            accumulated_time += est_duration
                        
                        time_ptr[0] += total_duration

                except Exception as e:
                    print(f"[Error] Batch generation failed: {e}")
                    current_full_wav.append(torch.zeros(1, 24000))
                    # Fallback timestamps
                    start_t = time_ptr[0]
                    end_t = start_t + 1.0
                    for item in batch_items:
                        segments_info.append({
                            "start": start_t, "end": end_t, "text": item["text"] + " (Gen Failed)", "speaker": item["speaker"]
                        })
                    time_ptr[0] += 1.0
                    
            else:
                # CosyVoice (Iterative)
                for i, item in enumerate(batch_items):
                    spk_name = item["speaker"]
                    spk_key = get_speaker_key(spk_name)
                    text = item["text"]
                    emotion = item.get("emotion", "None")
                    
                    spk_id = kwargs.get(f"speaker_{spk_key}_id", "")
                    ref_audio = get_ref_audio(spk_key)
                    
                    instruct = f"{emotion}." if emotion and emotion != "None" else ""
                    
                    print(f"  [Processing] {spk_name}: {text[:15]}...")
                    try:
                        res = cosy_gen.generate(
                            model=cosyvoice_model,
                            tts_text=text,
                            instruct_text=instruct,
                            spk_id=spk_id,
                            speed=speed_global,
                            seed=42+i,
                            dialect="None (Auto)",
                            emotion="None (Neutral)",
                            reference_audio=ref_audio
                        )
                        generated = res[0]
                        wav = generated["waveform"]
                        sr = generated["sample_rate"]
                        
                        if sr_ptr[0] != sr:
                            if current_full_wav:
                                wav = torchaudio.transforms.Resample(sr, sr_ptr[0])(wav)
                            else:
                                sr_ptr[0] = sr
                        
                        if wav.ndim == 3: wav = wav.squeeze(0)
                        if wav.ndim == 1: wav = wav.unsqueeze(0)
                        current_full_wav.append(wav)

                        # --- Timestamp Tracking ---
                        seg_duration = wav.shape[-1] / sr
                        seg_start = time_ptr[0]
                        seg_end = seg_start + seg_duration
                        
                        segments_info.append({
                            "start": round(seg_start, 3),
                            "end": round(seg_end, 3),
                            "text": text,
                            "speaker": spk_name,
                            "visual": item.get("visual")
                        })
                        time_ptr[0] += seg_duration
                        
                        if tts_engine == "CosyVoice" and i < len(batch_items) - 1:
                            gap = 0.2
                            gap_samples = int(gap * sr_ptr[0])
                            current_full_wav.append(torch.zeros(1, gap_samples))
                            time_ptr[0] += gap
                            
                    except Exception as e:
                        print(f"[Error] Item generation failed: {e}")
                        current_full_wav.append(torch.zeros(1, 16000))
                        time_ptr[0] += 1.0

                # --- Main Loop ---
        batch_buffer = []
        sr_wrapper = [22050 if tts_engine == "CosyVoice" else 24000] # Mutable ref
        
        for i, item in enumerate(dialogue):
            # Check mode
            batch_mode = kwargs.get("batch_mode", "Natural (Hybrid)")

            if item["type"] == "pause":
                # In Whole Batch mode, we IGNORE pauses to maintain single-batch integrity
                if batch_mode == "Whole (Single Batch)":
                    print("  [Whole Batch] Ignoring explicit pause to maintain flow.")
                    continue

                # Flush current speech batch
                flush_batch(batch_buffer, full_waveform, sr_wrapper)
                batch_buffer = []
                
                # Append Pause
                duration = item.get("duration", pause_duration)
                print(f"  [Pause] {duration}s")
                silence_samples = int(duration * sr_wrapper[0])
                if silence_samples > 0:
                    full_waveform.append(torch.zeros(1, silence_samples))
                    time_ptr[0] += duration # Track pause time

            
            elif item["type"] == "speech":
                # Check for strict mode
                batch_mode = kwargs.get("batch_mode", "Natural (Hybrid)")
                
                # If strict mode, flush on speaker change
                if batch_mode == "Strict (Per-Speaker)" and batch_buffer:
                    last_spk = batch_buffer[-1]["speaker"]
                    curr_spk = item["speaker"]
                    # Simple name check or mapped key check? 
                    # Let's use name check for simplicity as key derivation is same
                    if last_spk != curr_spk:
                        flush_batch(batch_buffer, full_waveform, sr_wrapper)
                        batch_buffer = []

                batch_buffer.append(item)
        
        # Final Flush
        flush_batch(batch_buffer, full_waveform, sr_wrapper)

        # Merge
        sample_rate = sr_wrapper[0]
        if not full_waveform:
            final_tensor = torch.zeros(1, 16000)
            sample_rate = 16000
        else:
            final_tensor = torch.cat(full_waveform, dim=1)
        
        # Output Segments Info JSON
        segments_json = json.dumps(segments_info, ensure_ascii=False, indent=2)

        return ({"waveform": final_tensor.unsqueeze(0), "sample_rate": sample_rate}, segments_json)


# ËäÇÁÇπÊò†Â∞ÑÂØºÂá∫
NODE_CLASS_MAPPINGS = {
    "AIIA_Podcast_Script_Parser": AIIA_Podcast_Script_Parser,
    "AIIA_Dialogue_TTS": AIIA_Dialogue_TTS,
    "AIIA_Segment_Merge": AIIA_Segment_Merge
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "AIIA_Podcast_Script_Parser": "üìú AIIA Podcast Script Parser",
    "AIIA_Dialogue_TTS": "üéß AIIA Dialogue TTS (Multi-Role)",
    "AIIA_Segment_Merge": "üîó AIIA Segment Merge (Visual)"
}
